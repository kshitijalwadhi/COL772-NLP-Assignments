{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/labels.json') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the labels\n",
    "labels_inv = {v: k for k, v in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.readlines()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file('Data/train.txt')\n",
    "val_data = read_file('Data/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1252586/1252586 [00:33<00:00, 37630.26it/s]\n"
     ]
    }
   ],
   "source": [
    "w2v_file = read_file(\"W2V/patent_w2v.txt\")\n",
    "def get_w2v_embeddings(w2v_file):\n",
    "    w2v_embeddings = {}\n",
    "    for line in tqdm(w2v_file[1:]):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        for i in range(1, len(values)):\n",
    "            values[i] = float(values[i])\n",
    "        vector = np.asarray(values[1:], 'float32')\n",
    "        w2v_embeddings[word]=vector\n",
    "    return w2v_embeddings\n",
    "embeddings = get_w2v_embeddings(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_KEY = \"<numeric>\"\n",
    "UNK_KEY = \"<unk>\"\n",
    "\n",
    "ADDITIONAL_KEYS = [NUMERIC_KEY, UNK_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ADDITIONAL_KEYS:\n",
    "    embeddings[k] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_keys = list(embeddings.keys())\n",
    "vocab_keys = []\n",
    "vocab_keys.append(\"<unk>\")\n",
    "vocab_keys.append(\"<pad>\")\n",
    "vocab_keys.append(\"<numeric>\")\n",
    "vocab = {k: v for v, k in enumerate(vocab_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_vocab(data):\n",
    "    vocab = {}\n",
    "    num_words = 0\n",
    "    for line in data:\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = build_train_vocab(train_data)\n",
    "# extend the vocab with the train_vocab\n",
    "idx = len(vocab)\n",
    "for word in train_vocab:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7400"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quantity(word):\n",
    "    # check if the word is a quantity\n",
    "    if re.match(r'^\\d+\\.?\\d*[a-zA-Z]*$', word):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantity_vector(word):\n",
    "    num = re.findall(r'\\d+\\.?\\d*', word)\n",
    "    if len(num) > 0:\n",
    "        num = float(num[0])\n",
    "    else:\n",
    "        num = 0\n",
    "    unit = re.findall(r'[a-zA-Z]+', word)\n",
    "    if len(unit) > 0:\n",
    "        unit = unit[0]\n",
    "    else:\n",
    "        unit = \"\"\n",
    "    if unit in embeddings:\n",
    "        return embeddings[\"<numeric>\"] + embeddings[unit]\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    word = word.replace(\"~\",\"\")\n",
    "    temp = word.replace(\",\", \"\")\n",
    "    temp = temp.replace(\"-\", \"\")\n",
    "    if temp.replace(\".\", \"\", 1).isdigit():\n",
    "        return embeddings[NUMERIC_KEY]\n",
    "    elif word in embeddings:\n",
    "        return embeddings[word]\n",
    "    elif check_if_quantity(word):\n",
    "        return get_quantity_vector(word)\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab)\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    # weights_matrix[i] = get_vector(word)\n",
    "    if word in embeddings:\n",
    "        weights_matrix[i] = embeddings[word]\n",
    "    else:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = torch.from_numpy(weights_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    sent_labels = []\n",
    "    all_labels = []\n",
    "    sent_idx = []\n",
    "    all_idx = []\n",
    "    for line in (data):\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            tag = split_line[1]\n",
    "            tag = tag.replace(\"\\n\", \"\")\n",
    "            word = word.lower()\n",
    "            if word in vocab:\n",
    "                sent_idx.append(vocab[word])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<unk>\"])\n",
    "            tag_idx = labels[tag]\n",
    "            sent_labels.append(tag_idx)\n",
    "        elif line==\"\\n\":\n",
    "            sent_idx = np.array(sent_idx)\n",
    "            sent_labels = np.array(sent_labels)\n",
    "            all_idx.append(sent_idx)\n",
    "            all_labels.append(sent_labels)\n",
    "            sent_idx = []\n",
    "            sent_labels = []\n",
    "        else:\n",
    "            print(line)\n",
    "    return np.asarray(all_idx, dtype=object), np.asarray(all_labels, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = get_data(train_data)\n",
    "valX, valY = get_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = []\n",
    "valData = []\n",
    "for i in range(len(trainX)):\n",
    "    trainData.append((trainX[i], trainY[i]))\n",
    "for i in range(len(valX)):\n",
    "    valData.append((valX[i], valY[i]))\n",
    "trainData = np.array(trainData, dtype=object)\n",
    "valData = np.array(valData, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    \n",
    "    batch_size = len(data)\n",
    "    \n",
    "    max_len = -1\n",
    "    for i in range(batch_size):\n",
    "        if len(data[i][0]) > max_len:\n",
    "            max_len = len(data[i][0])\n",
    "    \n",
    "    seq_lengths = []\n",
    "    for i in range(batch_size):\n",
    "        seq_lengths.append(len(data[i][0]))\n",
    "    \n",
    "    padded_data = []\n",
    "    padded_labels = []\n",
    "    mask = []\n",
    "    for i in range(batch_size):\n",
    "        padded_data.append(np.pad(data[i][0], (0, max_len-len(data[i][0])), 'constant', constant_values=(vocab[\"<pad>\"])))\n",
    "        padded_labels.append(np.pad(data[i][1], (0, max_len-len(data[i][1])), 'constant', constant_values=[\"37\"]))\n",
    "        mask.append(np.pad(np.ones(len(data[i][0])), (0, max_len-len(data[i][0])), 'constant', constant_values=0).astype(bool))\n",
    "    \n",
    "    padded_data = torch.from_numpy(np.array(padded_data))\n",
    "    padded_labels = torch.from_numpy(np.array(padded_labels))\n",
    "    mask = torch.from_numpy(np.array(mask))\n",
    "\n",
    "    return [padded_data, padded_labels, seq_lengths, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainDataLoader:\n",
    "    X, y, seq_lens, mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_dim, tagset_size):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze=False)\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        # self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence, labels, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return -self.crf(emissions, labels, mask=mask)\n",
    "\n",
    "    def predict(self, sentence, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        scores = self.hidden2tag(lstm_out)\n",
    "        return self.crf.decode(scores, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        X, y, seq_lens, mask = batch\n",
    "        loss = model(X, y, mask)\n",
    "        predictions = model.predict(X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(preds, gold):\n",
    "    flatten_preds = []\n",
    "    flatten_gold = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            flatten_preds.append(preds[i][j])\n",
    "            flatten_gold.append(gold[i][j])\n",
    "    idx = np.where(np.array(flatten_gold) != 0)[0]\n",
    "    micro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='micro')\n",
    "    macro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='macro')\n",
    "    return micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=37)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in (range(epochs)):\n",
    "        print(\"Training Epoch {}\".format(epoch))\n",
    "        training_loss = train_one_epoch(model, trainDataLoader, optimizer, loss_function)\n",
    "        print(\"Training Loss: {}\".format(training_loss))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        train_preds = []\n",
    "        for batch in trainDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            train_preds.extend(predictions)\n",
    "        train_preds = np.array(train_preds, dtype=object)\n",
    "\n",
    "        train_micro_f1, train_macro_f1 = get_scores(train_preds, trainY)\n",
    "\n",
    "        val_preds = []\n",
    "        for batch in valDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            val_preds.extend(predictions)\n",
    "        val_preds = np.array(val_preds, dtype=object)\n",
    "\n",
    "        val_micro_f1, val_macro_f1 = get_scores(val_preds, valY)\n",
    "\n",
    "        print(\"Training Micro F1: {}\".format(train_micro_f1))\n",
    "        print(\"Training Macro F1: {}\".format(train_macro_f1))\n",
    "        print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "        print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "\n",
    "        train_f1 = (train_micro_f1 + train_macro_f1) / 2\n",
    "        val_f1 = (val_micro_f1 + val_macro_f1) / 2\n",
    "\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            print(\"New Best Model at Epoch {}\".format(epoch))\n",
    "            print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "            print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        \n",
    "        if epoch>=best_epoch + 3:\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    return model, train_f1s, val_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = BiLSTMCRF(weights_matrix, 256, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:35<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 701.8913517252604\n",
      "Training Micro F1: 0.5908293935314193\n",
      "Training Macro F1: 0.3521464893854044\n",
      "Validation Micro F1: 0.5181200176064948\n",
      "Validation Macro F1: 0.31601508382971394\n",
      "New Best Model at Epoch 0\n",
      "Validation Micro F1: 0.5181200176064948\n",
      "Validation Macro F1: 0.31601508382971394\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:35<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 423.997747124566\n",
      "Training Micro F1: 0.6684930649049751\n",
      "Training Macro F1: 0.5018955487330797\n",
      "Validation Micro F1: 0.5733848486330513\n",
      "Validation Macro F1: 0.4461281038749001\n",
      "New Best Model at Epoch 1\n",
      "Validation Micro F1: 0.5733848486330513\n",
      "Validation Macro F1: 0.4461281038749001\n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 352.8575948079427\n",
      "Training Micro F1: 0.7104480415792604\n",
      "Training Macro F1: 0.5792843978570342\n",
      "Validation Micro F1: 0.5954907810436739\n",
      "Validation Macro F1: 0.5096407090655473\n",
      "New Best Model at Epoch 2\n",
      "Validation Micro F1: 0.5954907810436739\n",
      "Validation Macro F1: 0.5096407090655473\n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 305.7823756917318\n",
      "Training Micro F1: 0.7366542471586461\n",
      "Training Macro F1: 0.6155601989476462\n",
      "Validation Micro F1: 0.6078642343620091\n",
      "Validation Macro F1: 0.5318397594251538\n",
      "New Best Model at Epoch 3\n",
      "Validation Micro F1: 0.6078642343620091\n",
      "Validation Macro F1: 0.5318397594251538\n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 270.1366017659505\n",
      "Training Micro F1: 0.7630796205266289\n",
      "Training Macro F1: 0.6531222763096136\n",
      "Validation Micro F1: 0.6174499926639605\n",
      "Validation Macro F1: 0.5502004454772701\n",
      "New Best Model at Epoch 4\n",
      "Validation Micro F1: 0.6174499926639605\n",
      "Validation Macro F1: 0.5502004454772701\n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 241.45596455891928\n",
      "Training Micro F1: 0.7830551989730423\n",
      "Training Macro F1: 0.684747996623141\n",
      "Validation Micro F1: 0.6202376876803443\n",
      "Validation Macro F1: 0.5549680092710757\n",
      "New Best Model at Epoch 5\n",
      "Validation Micro F1: 0.6202376876803443\n",
      "Validation Macro F1: 0.5549680092710757\n",
      "Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 217.9472102186415\n",
      "Training Micro F1: 0.8029994677353706\n",
      "Training Macro F1: 0.7187061656594725\n",
      "Validation Micro F1: 0.6262043331540079\n",
      "Validation Macro F1: 0.5647671580072361\n",
      "New Best Model at Epoch 6\n",
      "Validation Micro F1: 0.6262043331540079\n",
      "Validation Macro F1: 0.5647671580072361\n",
      "Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 197.44150319417318\n",
      "Training Micro F1: 0.8186386549359717\n",
      "Training Macro F1: 0.7406751096316901\n",
      "Validation Micro F1: 0.6268890301755758\n",
      "Validation Macro F1: 0.5670369771806291\n",
      "New Best Model at Epoch 7\n",
      "Validation Micro F1: 0.6268890301755758\n",
      "Validation Macro F1: 0.5670369771806291\n",
      "Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 178.04374047173394\n",
      "Training Micro F1: 0.8375183944394002\n",
      "Training Macro F1: 0.7627680588508928\n",
      "Validation Micro F1: 0.6265955885949039\n",
      "Validation Macro F1: 0.5667955184151072\n",
      "Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:38<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 160.5964633517795\n",
      "Training Micro F1: 0.854973543316948\n",
      "Training Macro F1: 0.7810440002560258\n",
      "Validation Micro F1: 0.6263021470142319\n",
      "Validation Macro F1: 0.5681951553772734\n",
      "New Best Model at Epoch 9\n",
      "Validation Micro F1: 0.6263021470142319\n",
      "Validation Macro F1: 0.5681951553772734\n",
      "Training Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:38<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 143.616334499783\n",
      "Training Micro F1: 0.8752778734462568\n",
      "Training Macro F1: 0.8058845438233927\n",
      "Validation Micro F1: 0.624688218320536\n",
      "Validation Macro F1: 0.562903551299026\n",
      "Training Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 126.6094692993164\n",
      "Training Micro F1: 0.8987601365102226\n",
      "Training Macro F1: 0.8408620233958692\n",
      "Validation Micro F1: 0.6268890301755758\n",
      "Validation Macro F1: 0.5683359908960649\n",
      "New Best Model at Epoch 11\n",
      "Validation Micro F1: 0.6268890301755758\n",
      "Validation Macro F1: 0.5683359908960649\n",
      "Training Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 109.27097744411893\n",
      "Training Micro F1: 0.9144932527630796\n",
      "Training Macro F1: 0.859049206608165\n",
      "Validation Micro F1: 0.6230742896268401\n",
      "Validation Macro F1: 0.562653780384435\n",
      "Training Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 94.40848739624023\n",
      "Training Micro F1: 0.9290052913366103\n",
      "Training Macro F1: 0.8805823542462607\n",
      "Validation Micro F1: 0.6235633589279601\n",
      "Validation Macro F1: 0.5616466842785937\n",
      "Training Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 80.76393747965494\n",
      "Training Micro F1: 0.9359716960455869\n",
      "Training Macro F1: 0.8927338437048564\n",
      "Validation Micro F1: 0.6184770381963124\n",
      "Validation Macro F1: 0.5493059976613026\n"
     ]
    }
   ],
   "source": [
    "ner, train_f1s, val_f1s = train_model(ner, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model\n",
    "ner.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMCRF(\n",
       "  (embedding): Embedding(7400, 200)\n",
       "  (lstm): LSTM(200, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=38, bias=True)\n",
       "  (crf): CRF(num_tags=38)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation predictions using valDataloader\n",
    "val_preds = []\n",
    "for batch in valDataLoader:\n",
    "    X, y, seq_lens, mask = batch\n",
    "    predictions = ner.predict(X, mask)\n",
    "    val_preds.extend(predictions)\n",
    "val_preds = np.array(val_preds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_val_preds = []\n",
    "flatten_valY = []\n",
    "for i in range(len(val_preds)):\n",
    "    for j in range(len(val_preds[i])):\n",
    "        flatten_val_preds.append(val_preds[i][j])\n",
    "        flatten_valY.append(valY[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564635590186954"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(flatten_valY, flatten_val_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5750447567361079"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(flatten_valY, flatten_val_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6268890301755758"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx where flatten_valY is not 0\n",
    "idx = np.where(np.array(flatten_valY) != 0)[0]\n",
    "f1_score(np.array(flatten_valY)[idx], np.array(flatten_val_preds)[idx], average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5683359908960649"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.array(flatten_valY)[idx], np.array(flatten_val_preds)[idx], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6, 14, 15, 2, 3, 0, 0, 22, 0, 0, 26, 0]),\n",
       "       list([6, 2, 0, 4, 0, 6, 14, 15, 0, 16, 17, 2, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 0, 0]),\n",
       "       list([6, 14, 15, 26, 27, 0, 2, 3, 0]), ...,\n",
       "       list([6, 0, 22, 0, 11, 12, 0, 6, 2, 0, 0, 22, 0]),\n",
       "       list([6, 4, 0, 6, 0, 6, 0, 22, 0]),\n",
       "       list([6, 0, 18, 19, 0, 11, 12, 0, 0, 0, 4, 6, 0, 6, 0, 2, 0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "with open('val_preds.txt', 'w') as f:\n",
    "    for i in range(len(val_preds)):\n",
    "        for j in range(len(val_preds[i])):\n",
    "            f.write(labels_inv[val_preds[i][j]] + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION Report\n",
      "                 precision    recall  f1_score  true_entities  pred_entities\n",
      "Reagent           0.795750  0.766342  0.780769         5033.0         4847.0\n",
      "Action            0.834977  0.790934  0.812359         3640.0         3448.0\n",
      "Modifier          0.316424  0.380881  0.345673         1998.0         2405.0\n",
      "Location          0.743812  0.604324  0.666852         1989.0         1616.0\n",
      "Amount            0.868325  0.832945  0.850267         1718.0         1648.0\n",
      "Time              0.888889  0.866242  0.877419         1099.0         1071.0\n",
      "Device            0.558859  0.520487  0.538991          903.0          841.0\n",
      "Method            0.560633  0.353659  0.433719          902.0          569.0\n",
      "Concentration     0.751857  0.714689  0.732802          708.0          673.0\n",
      "Temperature       0.904048  0.900000  0.902019          670.0          667.0\n",
      "Measure-Type      0.560000  0.455814  0.502564          430.0          350.0\n",
      "Generic-Measure   0.439189  0.216667  0.290179          300.0          148.0\n",
      "Speed             0.900000  0.752508  0.819672          299.0          250.0\n",
      "Numerical         0.511905  0.496154  0.503906          260.0          252.0\n",
      "Size              0.723684  0.497738  0.589812          221.0          152.0\n",
      "Seal              0.755319  0.563492  0.645455          126.0           94.0\n",
      "Mention           0.671053  0.593023  0.629630           86.0           76.0\n",
      "pH                0.979167  0.723077  0.831858           65.0           48.0\n",
      "micro_avg         0.724302  0.678535  0.700672        20447.0        19155.0\n",
      "macro_avg         0.709105  0.612721  0.652997        20447.0        19155.0\n",
      "weighted_avg      0.729898  0.678535  0.700783        20447.0        19155.0\n",
      "\n",
      "F scores\n",
      "Micro-F1 = 0.7006716832483207\n",
      "Macro-F1 = 0.6529970555104652\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py Data/dev.txt val_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
