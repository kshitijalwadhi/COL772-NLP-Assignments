{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10779b410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/labels.json') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_labels = {\n",
    "    \"START_OF_SENTENCE\": 37,\n",
    "    \"END_OF_SENTENCE\": 38,\n",
    "    \"PAD\": 39\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.update(additional_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_line_by_line(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.readlines()\n",
    "    return text\n",
    "train_data = read_file_line_by_line('Data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    vocab = {}\n",
    "    num_words = 0\n",
    "    for line in data:\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            if word not in vocab:\n",
    "                vocab[word] = num_words\n",
    "                num_words += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmbeddings = {}\n",
    "with open('glove.6B/glove.6B.200d.txt','r') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    emmbeddings[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, max_length = 150):\n",
    "    sent_labels = []\n",
    "    all_labels = []\n",
    "    sent_embeddings = []\n",
    "    all_embeddings = []\n",
    "    for line in (data):\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            tag = split_line[1]\n",
    "            tag = tag.replace(\"\\n\", \"\")\n",
    "            if word in emmbeddings:\n",
    "                embed = np.array(emmbeddings[word])\n",
    "                sent_embeddings.append(embed)\n",
    "            else:\n",
    "                sent_embeddings.append(np.zeros(200))\n",
    "            tag_idx = labels[tag]\n",
    "            sent_labels.append(tag_idx)\n",
    "        elif line==\"\\n\":\n",
    "            # if len(sent_embeddings) < max_length:\n",
    "            #     for i in range(max_length - len(sent_embeddings)):\n",
    "            #         sent_embeddings.append(np.zeros(200))\n",
    "            #         sent_labels.append(38)\n",
    "            sent_embeddings = np.array(sent_embeddings)\n",
    "            sent_labels = np.array(sent_labels)\n",
    "            all_embeddings.append(sent_embeddings)\n",
    "            all_labels.append(sent_labels)\n",
    "            sent_embeddings = []\n",
    "            sent_labels = []\n",
    "    return np.asarray(all_embeddings), np.asarray(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/9kgm8vks4wz6zr8rg0d_pb7w0000gn/T/ipykernel_3993/63509286.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(all_embeddings), np.asarray(all_labels)\n"
     ]
    }
   ],
   "source": [
    "train_data_embed, train_labels = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7198,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_yet = -1\n",
    "for i in range(len(train_data_embed)):\n",
    "    if len(train_data_embed[i]) > max_yet:\n",
    "        max_yet = len(train_data_embed[i])\n",
    "max_yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.word_embedding = nn.from_pretrained_embeddings()    \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_dim),\n",
    "                torch.zeros(2, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # sentence: [[size 200],[size 200],[size 200]....]\n",
    "        input_tensor = torch.tensor(sentence, dtype=torch.float)\n",
    "        lstm_out, _ = self.lstm(sentence)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, epochs):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in (range(epochs)):\n",
    "        for sentence, tags in tqdm(zip(train_data, train_labels)):\n",
    "            model.zero_grad()\n",
    "            model.hidden = model.init_hidden()\n",
    "            # convert sentence to tensor\n",
    "            sentence_in = torch.tensor(sentence, dtype=torch.float)\n",
    "            tag_scores = model(sentence_in)\n",
    "            # convert tags to tensor\n",
    "            tags = torch.tensor(tags, dtype=torch.long)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = BiLSTM(200, 64, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/93/9kgm8vks4wz6zr8rg0d_pb7w0000gn/T/ipykernel_3993/1495971710.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(sentence, dtype=torch.float)\n",
      "7198it [00:26, 275.53it/s]\n",
      "7198it [00:25, 277.63it/s]\n",
      "7198it [00:26, 272.19it/s]\n",
      "7198it [00:25, 277.08it/s]\n",
      "7198it [00:25, 277.72it/s]\n",
      "7198it [00:26, 275.22it/s]\n",
      "7198it [00:26, 272.57it/s]\n",
      "7198it [00:25, 284.44it/s]\n",
      "7198it [00:26, 268.92it/s]\n",
      "7198it [00:27, 264.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_data_tensor = torch.tensor(train_data_embed, dtype=torch.float)\n",
    "# train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "# to device\n",
    "# ner = ner.to(device)\n",
    "# train_data_tensor = train_data_tensor.to(device)\n",
    "# train_labels_tensor = train_labels_tensor.to(device)\n",
    "\n",
    "trained_model = train_model(ner, train_data_embed, train_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/9kgm8vks4wz6zr8rg0d_pb7w0000gn/T/ipykernel_3993/1495971710.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(sentence, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 2, 0, 0, 6, 0, 2, 0, 0, 0, 0, 6, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor(train_data_embed[0], dtype=torch.float)\n",
    "out = ner(inp)\n",
    "\n",
    "# take the argmax of the output\n",
    "out = out.detach().numpy()\n",
    "out = np.argmax(out, axis=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  2,  3,  0,  6, 13,  0,  2,  0, 11, 12,  0,  0,  2,  3,  6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/9kgm8vks4wz6zr8rg0d_pb7w0000gn/T/ipykernel_3993/1495971710.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(sentence, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(train_data_embed)):\n",
    "    inp = torch.tensor(train_data_embed[i], dtype=torch.float)\n",
    "    out = ner(inp)\n",
    "    # take the argmax of the output\n",
    "    out = out.detach().numpy()\n",
    "    out = np.argmax(out, axis=1)\n",
    "    gold = train_labels[i]\n",
    "\n",
    "    correct += np.sum(out == gold)\n",
    "    total += len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.68093242675768"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
