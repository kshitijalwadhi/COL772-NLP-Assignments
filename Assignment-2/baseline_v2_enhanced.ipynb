{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/labels.json') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the labels\n",
    "labels_inv = {v: k for k, v in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.readlines()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file('Data/train.txt')\n",
    "val_data = read_file('Data/dev.txt')\n",
    "test_data = read_file(\"Data/test.txt\")\n",
    "test_data2 = read_file(\"Data/test_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "emb_dim = 50\n",
    "with open('glove.6B/glove.6B.50d.txt','r') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    embeddings[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_KEY = \"<numeric>\"\n",
    "UNK_KEY = \"<unk>\"\n",
    "\n",
    "ADDITIONAL_KEYS = [NUMERIC_KEY, UNK_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ADDITIONAL_KEYS:\n",
    "    embeddings[k] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_keys = list(embeddings.keys())\n",
    "vocab_keys = []\n",
    "vocab_keys.append(\"<unk>\")\n",
    "vocab_keys.append(\"<pad>\")\n",
    "vocab_keys.append(\"<numeric>\")\n",
    "vocab_keys.append(\"<conc>\")\n",
    "vocab = {k: v for v, k in enumerate(vocab_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_vocab(data):\n",
    "    vocab = {}\n",
    "    num_words = 0\n",
    "    for line in data:\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data + val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = build_train_vocab(train_data)\n",
    "# extend the vocab with the train_vocab\n",
    "idx = len(vocab)\n",
    "for word in train_vocab:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8548"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump vocab\n",
    "with open('vocab.json', 'w') as fp:\n",
    "    json.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quantity(word):\n",
    "    # check if the word is a quantity\n",
    "    if re.match(r'^\\d+\\.?\\d*[a-zA-Z]*$', word):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantity_vector(word):\n",
    "    num = re.findall(r'\\d+\\.?\\d*', word)\n",
    "    if len(num) > 0:\n",
    "        num = float(num[0])\n",
    "    else:\n",
    "        num = 0\n",
    "    unit = re.findall(r'[a-zA-Z]+', word)\n",
    "    if len(unit) > 0:\n",
    "        unit = unit[0]\n",
    "    else:\n",
    "        unit = \"\"\n",
    "    if unit in embeddings:\n",
    "        return embeddings[\"<numeric>\"] + embeddings[unit]\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_conc(word):\n",
    "    # check if word is a concentration\n",
    "    if re.match(r'[a-zA-Z]*\\/[a-zA-Z]*', word):\n",
    "        return True\n",
    "    elif word == \"%\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    word = word.replace(\"~\",\"\")\n",
    "    temp = word.replace(\",\", \"\")\n",
    "    temp = temp.replace(\"-\", \"\")\n",
    "    if temp.replace(\".\", \"\", 1).isdigit():\n",
    "        return embeddings[NUMERIC_KEY]\n",
    "    elif check_if_conc(word):\n",
    "        return embeddings[\"<conc>\"]\n",
    "    elif word in embeddings:\n",
    "        return embeddings[word]\n",
    "    elif check_if_quantity(word):\n",
    "        return get_quantity_vector(word)\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab)\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    #weights_matrix[i] = get_vector(word)\n",
    "    if word in embeddings:\n",
    "        weights_matrix[i] = embeddings[word]\n",
    "    else:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = torch.from_numpy(weights_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    sent_labels = []\n",
    "    all_labels = []\n",
    "    sent_idx = []\n",
    "    all_idx = []\n",
    "    for line in (data):\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            tag = split_line[1]\n",
    "            tag = tag.replace(\"\\n\", \"\")\n",
    "            word = word.lower()\n",
    "            if word in vocab:\n",
    "                sent_idx.append(vocab[word])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<unk>\"])\n",
    "            tag_idx = labels[tag]\n",
    "            sent_labels.append(tag_idx)\n",
    "        elif line==\"\\n\":\n",
    "            sent_idx = np.array(sent_idx)\n",
    "            sent_labels = np.array(sent_labels)\n",
    "            all_idx.append(sent_idx)\n",
    "            all_labels.append(sent_labels)\n",
    "            sent_idx = []\n",
    "            sent_labels = []\n",
    "        else:\n",
    "            print(line)\n",
    "    return np.asarray(all_idx, dtype=object), np.asarray(all_labels, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = get_data(train_data)\n",
    "valX, valY = get_data(val_data)\n",
    "test1X, test1Y = get_data(test_data)\n",
    "test2X, test2Y = get_data(test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "test1X, test1Y = shuffle(test1X, test1Y, random_state=42)\n",
    "test2X, test2Y = shuffle(test2X, test2Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend trainX with test1X and test2X\n",
    "trainX = np.concatenate((trainX, test1X[:2000], test2X[:2000]), axis=0)\n",
    "trainY = np.concatenate((trainY, test1Y[:2000], test2Y[:2000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = test1X\n",
    "testY = test1Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = []\n",
    "valData = []\n",
    "testData = []\n",
    "for i in range(len(trainX)):\n",
    "    trainData.append((trainX[i], trainY[i]))\n",
    "for i in range(len(valX)):\n",
    "    valData.append((valX[i], valY[i]))\n",
    "for i in range(len(testX)):\n",
    "    testData.append((testX[i], testY[i]))\n",
    "trainData = np.array(trainData, dtype=object)\n",
    "valData = np.array(valData, dtype=object)\n",
    "testData = np.array(testData, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    \n",
    "    batch_size = len(data)\n",
    "    \n",
    "    max_len = -1\n",
    "    for i in range(batch_size):\n",
    "        if len(data[i][0]) > max_len:\n",
    "            max_len = len(data[i][0])\n",
    "    \n",
    "    seq_lengths = []\n",
    "    for i in range(batch_size):\n",
    "        seq_lengths.append(len(data[i][0]))\n",
    "    \n",
    "    padded_data = []\n",
    "    padded_labels = []\n",
    "    mask = []\n",
    "    for i in range(batch_size):\n",
    "        padded_data.append(np.pad(data[i][0], (0, max_len-len(data[i][0])), 'constant', constant_values=(vocab[\"<pad>\"])))\n",
    "        padded_labels.append(np.pad(data[i][1], (0, max_len-len(data[i][1])), 'constant', constant_values=[\"37\"]))\n",
    "        mask.append(np.pad(np.ones(len(data[i][0])), (0, max_len-len(data[i][0])), 'constant', constant_values=0).astype(bool))\n",
    "    \n",
    "    padded_data = torch.from_numpy(np.array(padded_data))\n",
    "    padded_labels = torch.from_numpy(np.array(padded_labels))\n",
    "    mask = torch.from_numpy(np.array(mask))\n",
    "\n",
    "    return [padded_data, padded_labels, seq_lengths, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainDataLoader:\n",
    "    X, y, seq_lens, mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_dim, tagset_size):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze=False)\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        # self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence, labels, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return -self.crf(emissions, labels, mask=mask)\n",
    "\n",
    "    def predict(self, sentence, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        scores = self.hidden2tag(lstm_out)\n",
    "        return self.crf.decode(scores, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        X, y, seq_lens, mask = batch\n",
    "        loss = model(X, y, mask)\n",
    "        predictions = model.predict(X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(preds, gold):\n",
    "    flatten_preds = []\n",
    "    flatten_gold = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            flatten_preds.append(preds[i][j])\n",
    "            flatten_gold.append(gold[i][j])\n",
    "    idx = np.where(np.array(flatten_gold) != 0)[0]\n",
    "    micro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='micro')\n",
    "    macro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='macro')\n",
    "    return micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=37)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in (range(epochs)):\n",
    "        print(\"Training Epoch {}\".format(epoch))\n",
    "        training_loss = train_one_epoch(model, trainDataLoader, optimizer, loss_function)\n",
    "        print(\"Training Loss: {}\".format(training_loss))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        train_preds = []\n",
    "        for batch in trainDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            train_preds.extend(predictions)\n",
    "        train_preds = np.array(train_preds, dtype=object)\n",
    "\n",
    "        train_micro_f1, train_macro_f1 = get_scores(train_preds, trainY)\n",
    "\n",
    "        val_preds = []\n",
    "        for batch in testDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            val_preds.extend(predictions)\n",
    "        val_preds = np.array(val_preds, dtype=object)\n",
    "\n",
    "        val_micro_f1, val_macro_f1 = get_scores(val_preds, testY)\n",
    "\n",
    "        print(\"Training Micro F1: {}\".format(train_micro_f1))\n",
    "        print(\"Training Macro F1: {}\".format(train_macro_f1))\n",
    "        print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "        print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "\n",
    "        train_f1 = (train_micro_f1 + train_macro_f1) / 2\n",
    "        val_f1 = (val_micro_f1 + val_macro_f1) / 2\n",
    "\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            print(\"New Best Model at Epoch {}\".format(epoch))\n",
    "            print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "            print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model_2.pt')\n",
    "        \n",
    "        if epoch>=best_epoch + 3:\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    return model, train_f1s, val_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = BiLSTMCRF(weights_matrix, 256, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:11<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 687.565470715883\n",
      "Training Micro F1: 0.5682487320110511\n",
      "Training Macro F1: 0.37532870547812275\n",
      "Validation Micro F1: 0.5877438439398784\n",
      "Validation Macro F1: 0.3827465527626986\n",
      "New Best Model at Epoch 0\n",
      "Validation Micro F1: 0.5877438439398784\n",
      "Validation Macro F1: 0.3827465527626986\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:10<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 442.93714756184124\n",
      "Training Micro F1: 0.6514782895550699\n",
      "Training Macro F1: 0.49022810529211164\n",
      "Validation Micro F1: 0.6640150303805564\n",
      "Validation Macro F1: 0.4932199032966979\n",
      "New Best Model at Epoch 1\n",
      "Validation Micro F1: 0.6640150303805564\n",
      "Validation Macro F1: 0.4932199032966979\n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:10<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 360.6012865370073\n",
      "Training Micro F1: 0.6962434538781906\n",
      "Training Macro F1: 0.5553781388462004\n",
      "Validation Micro F1: 0.7037895746722098\n",
      "Validation Macro F1: 0.5531411752666571\n",
      "New Best Model at Epoch 2\n",
      "Validation Micro F1: 0.7037895746722098\n",
      "Validation Macro F1: 0.5531411752666571\n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:11<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 307.9717640638918\n",
      "Training Micro F1: 0.7217021978475114\n",
      "Training Macro F1: 0.5969014281884046\n",
      "Validation Micro F1: 0.7209385992964503\n",
      "Validation Macro F1: 0.5800306434618379\n",
      "New Best Model at Epoch 3\n",
      "Validation Micro F1: 0.7209385992964503\n",
      "Validation Macro F1: 0.5800306434618379\n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:10<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 272.11807087877867\n",
      "Training Micro F1: 0.7403900870067214\n",
      "Training Macro F1: 0.6305930089540499\n",
      "Validation Micro F1: 0.7334905660377359\n",
      "Validation Macro F1: 0.6037012330271807\n",
      "New Best Model at Epoch 4\n",
      "Validation Micro F1: 0.7334905660377359\n",
      "Validation Macro F1: 0.6037012330271807\n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:10<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 245.44087393210222\n",
      "Training Micro F1: 0.755416271493959\n",
      "Training Macro F1: 0.6561932654041195\n",
      "Validation Micro F1: 0.7443236328749601\n",
      "Validation Macro F1: 0.6259938239708429\n",
      "New Best Model at Epoch 5\n",
      "Validation Micro F1: 0.7443236328749601\n",
      "Validation Macro F1: 0.6259938239708429\n",
      "Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:11<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 225.40367840474687\n",
      "Training Micro F1: 0.7668879633829532\n",
      "Training Macro F1: 0.6736473212388933\n",
      "Validation Micro F1: 0.7535177486408698\n",
      "Validation Macro F1: 0.6417733235858464\n",
      "New Best Model at Epoch 6\n",
      "Validation Micro F1: 0.7535177486408698\n",
      "Validation Macro F1: 0.6417733235858464\n",
      "Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:12<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 209.14601796709727\n",
      "Training Micro F1: 0.7786730444105397\n",
      "Training Macro F1: 0.6906957749202605\n",
      "Validation Micro F1: 0.7603133994243685\n",
      "Validation Macro F1: 0.656676583214375\n",
      "New Best Model at Epoch 7\n",
      "Validation Micro F1: 0.7603133994243685\n",
      "Validation Macro F1: 0.656676583214375\n",
      "Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:12<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 196.29039476263267\n",
      "Training Micro F1: 0.788544802276195\n",
      "Training Macro F1: 0.7054214960617262\n",
      "Validation Micro F1: 0.7669491525423728\n",
      "Validation Macro F1: 0.6719020453773218\n",
      "New Best Model at Epoch 8\n",
      "Validation Micro F1: 0.7669491525423728\n",
      "Validation Macro F1: 0.6719020453773218\n",
      "Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:12<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 184.53266000634417\n",
      "Training Micro F1: 0.7998515525132985\n",
      "Training Macro F1: 0.7196659341511366\n",
      "Validation Micro F1: 0.7737048289094979\n",
      "Validation Macro F1: 0.676365267772749\n",
      "New Best Model at Epoch 9\n",
      "Validation Micro F1: 0.7737048289094979\n",
      "Validation Macro F1: 0.676365267772749\n",
      "Training Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:13<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 174.06242463028062\n",
      "Training Micro F1: 0.807809987217022\n",
      "Training Macro F1: 0.731423424175392\n",
      "Validation Micro F1: 0.779221298369044\n",
      "Validation Macro F1: 0.6860263295215774\n",
      "New Best Model at Epoch 10\n",
      "Validation Micro F1: 0.779221298369044\n",
      "Validation Macro F1: 0.6860263295215774\n",
      "Training Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:21<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 163.73352358856565\n",
      "Training Micro F1: 0.8225392767308566\n",
      "Training Macro F1: 0.7509206022036922\n",
      "Validation Micro F1: 0.7860169491525424\n",
      "Validation Macro F1: 0.7000575600657402\n",
      "New Best Model at Epoch 11\n",
      "Validation Micro F1: 0.7860169491525424\n",
      "Validation Macro F1: 0.7000575600657402\n",
      "Training Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:28<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 153.3626814998527\n",
      "Training Micro F1: 0.8346047585666571\n",
      "Training Macro F1: 0.7644282696814588\n",
      "Validation Micro F1: 0.7946514230892229\n",
      "Validation Macro F1: 0.709956530872118\n",
      "New Best Model at Epoch 12\n",
      "Validation Micro F1: 0.7946514230892229\n",
      "Validation Macro F1: 0.709956530872118\n",
      "Training Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:15<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 143.21144999234522\n",
      "Training Micro F1: 0.8501834975877284\n",
      "Training Macro F1: 0.7823852361980567\n",
      "Validation Micro F1: 0.801566997121842\n",
      "Validation Macro F1: 0.7166709992012198\n",
      "New Best Model at Epoch 13\n",
      "Validation Micro F1: 0.801566997121842\n",
      "Validation Macro F1: 0.7166709992012198\n",
      "Training Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:15<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 132.13836958062905\n",
      "Training Micro F1: 0.8626200981402828\n",
      "Training Macro F1: 0.7995944624896161\n",
      "Validation Micro F1: 0.8088423409018228\n",
      "Validation Macro F1: 0.7268464751027046\n",
      "New Best Model at Epoch 14\n",
      "Validation Micro F1: 0.8088423409018228\n",
      "Validation Macro F1: 0.7268464751027046\n",
      "Training Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:15<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 120.8475708587719\n",
      "Training Micro F1: 0.8766566327161767\n",
      "Training Macro F1: 0.821570332960966\n",
      "Validation Micro F1: 0.8133994243684042\n",
      "Validation Macro F1: 0.7320069692720843\n",
      "New Best Model at Epoch 15\n",
      "Validation Micro F1: 0.8133994243684042\n",
      "Validation Macro F1: 0.7320069692720843\n",
      "Training Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:23<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 109.7004601394762\n",
      "Training Micro F1: 0.8903880252360729\n",
      "Training Macro F1: 0.8367560833917694\n",
      "Validation Micro F1: 0.8194755356571796\n",
      "Validation Macro F1: 0.7377741304611523\n",
      "New Best Model at Epoch 16\n",
      "Validation Micro F1: 0.8194755356571796\n",
      "Validation Macro F1: 0.7377741304611523\n",
      "Training Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:23<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 98.70993022964007\n",
      "Training Micro F1: 0.8997154756504886\n",
      "Training Macro F1: 0.849266167272918\n",
      "Validation Micro F1: 0.822673488967061\n",
      "Validation Macro F1: 0.7441503674741822\n",
      "New Best Model at Epoch 17\n",
      "Validation Micro F1: 0.822673488967061\n",
      "Validation Macro F1: 0.7441503674741822\n",
      "Training Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:16<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 88.76597588872117\n",
      "Training Micro F1: 0.9095295039379819\n",
      "Training Macro F1: 0.8597050136154345\n",
      "Validation Micro F1: 0.8261512631915574\n",
      "Validation Macro F1: 0.7460627137362164\n",
      "New Best Model at Epoch 18\n",
      "Validation Micro F1: 0.8261512631915574\n",
      "Validation Macro F1: 0.7460627137362164\n",
      "Training Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:16<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 80.02194698632755\n",
      "Training Micro F1: 0.9184693414704548\n",
      "Training Macro F1: 0.8695366920349425\n",
      "Validation Micro F1: 0.8296290374160538\n",
      "Validation Macro F1: 0.7524586262248061\n",
      "New Best Model at Epoch 19\n",
      "Validation Micro F1: 0.8296290374160538\n",
      "Validation Macro F1: 0.7524586262248061\n",
      "Training Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:15<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 71.57847680841658\n",
      "Training Micro F1: 0.9269638365428231\n",
      "Training Macro F1: 0.8797766680499356\n",
      "Validation Micro F1: 0.8334266069715383\n",
      "Validation Macro F1: 0.7538021365275719\n",
      "New Best Model at Epoch 20\n",
      "Validation Micro F1: 0.8334266069715383\n",
      "Validation Macro F1: 0.7538021365275719\n",
      "Training Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:16<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 65.0120756054151\n",
      "Training Micro F1: 0.935408849119624\n",
      "Training Macro F1: 0.8931267696192693\n",
      "Validation Micro F1: 0.8368244323632875\n",
      "Validation Macro F1: 0.7716936634420214\n",
      "New Best Model at Epoch 21\n",
      "Validation Micro F1: 0.8368244323632875\n",
      "Validation Macro F1: 0.7716936634420214\n",
      "Training Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:17<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 59.33639846674903\n",
      "Training Micro F1: 0.9403241103459651\n",
      "Training Macro F1: 0.9034760258153338\n",
      "Validation Micro F1: 0.8379037416053725\n",
      "Validation Macro F1: 0.7660412892217672\n",
      "Training Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:24<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 54.203698654356025\n",
      "Training Micro F1: 0.9478619438373674\n",
      "Training Macro F1: 0.9087890633723622\n",
      "Validation Micro F1: 0.840701950751519\n",
      "Validation Macro F1: 0.7625892653540993\n",
      "Training Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [01:24<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 49.70047352716079\n",
      "Training Micro F1: 0.9484392396189848\n",
      "Training Macro F1: 0.9105536357830867\n",
      "Validation Micro F1: 0.8373440997761432\n",
      "Validation Macro F1: 0.7640473543107122\n"
     ]
    }
   ],
   "source": [
    "ner, train_f1s, val_f1s = train_model(ner, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model\n",
    "ner.load_state_dict(torch.load('best_model_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ner, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMCRF(\n",
       "  (embedding): Embedding(8548, 50)\n",
       "  (lstm): LSTM(50, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=38, bias=True)\n",
       "  (crf): CRF(num_tags=38)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation predictions using valDataloader\n",
    "val_preds = []\n",
    "for batch in valDataLoader:\n",
    "    X, y, seq_lens, mask = batch\n",
    "    predictions = ner.predict(X, mask)\n",
    "    val_preds.extend(predictions)\n",
    "val_preds = np.array(val_preds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_val_preds = []\n",
    "flatten_valY = []\n",
    "for i in range(len(val_preds)):\n",
    "    for j in range(len(val_preds[i])):\n",
    "        flatten_val_preds.append(val_preds[i][j])\n",
    "        flatten_valY.append(valY[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6, 14, 15, 2, 3, 0, 0, 22, 0, 0, 26, 0]),\n",
       "       list([6, 2, 0, 26, 0, 6, 14, 15, 0, 16, 17, 2, 0, 0, 0, 6, 0, 16, 17, 17, 2, 0, 0, 0]),\n",
       "       list([6, 13, 14, 0, 2, 3, 3, 3, 0]), ...,\n",
       "       list([6, 0, 22, 0, 11, 12, 0, 6, 2, 0, 0, 22, 0]),\n",
       "       list([6, 4, 0, 6, 0, 6, 0, 22, 0]),\n",
       "       list([6, 0, 18, 19, 0, 11, 12, 0, 0, 0, 4, 6, 0, 6, 0, 2, 0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "with open('val_preds.txt', 'w') as f:\n",
    "    for i in range(len(val_preds)):\n",
    "        for j in range(len(val_preds[i])):\n",
    "            f.write(labels_inv[val_preds[i][j]] + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION Report\n",
      "                 precision    recall  f1_score  true_entities  pred_entities\n",
      "Reagent           0.959399  0.939003  0.949091         5033.0         4926.0\n",
      "Action            0.927630  0.947253  0.937339         3640.0         3717.0\n",
      "Modifier          0.765049  0.852352  0.806345         1998.0         2226.0\n",
      "Location          0.947862  0.914027  0.930637         1989.0         1918.0\n",
      "Amount            0.983658  0.910943  0.945905         1718.0         1591.0\n",
      "Time              0.974600  0.942675  0.958372         1099.0         1063.0\n",
      "Device            0.923077  0.930233  0.926641          903.0          910.0\n",
      "Method            0.880044  0.894678  0.887301          902.0          917.0\n",
      "Concentration     0.889039  0.927966  0.908086          708.0          739.0\n",
      "Temperature       0.961708  0.974627  0.968125          670.0          679.0\n",
      "Measure-Type      0.740672  0.923256  0.821946          430.0          536.0\n",
      "Generic-Measure   0.986547  0.733333  0.841300          300.0          223.0\n",
      "Speed             1.000000  0.986622  0.993266          299.0          295.0\n",
      "Numerical         0.706231  0.915385  0.797320          260.0          337.0\n",
      "Size              0.856540  0.918552  0.886463          221.0          237.0\n",
      "Seal              0.929688  0.944444  0.937008          126.0          128.0\n",
      "Mention           0.868421  0.767442  0.814815           86.0           76.0\n",
      "pH                1.000000  0.953846  0.976378           65.0           62.0\n",
      "micro_avg         0.916084  0.922042  0.919053        20447.0        20580.0\n",
      "macro_avg         0.905565  0.909813  0.904796        20447.0        20580.0\n",
      "weighted_avg      0.920645  0.922042  0.920204        20447.0        20580.0\n",
      "\n",
      "F scores\n",
      "Micro-F1 = 0.9190533063592269\n",
      "Macro-F1 = 0.9047964748099532\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py Data/dev.txt val_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = get_data(test_data)\n",
    "testData = []\n",
    "for i in range(len(testX)):\n",
    "    testData.append((testX[i], testY[i]))\n",
    "testData = np.array(testData, dtype=object)\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation predictions using valDataloader\n",
    "test_preds = []\n",
    "for batch in testDataLoader:\n",
    "    X, y, seq_lens, mask = batch\n",
    "    predictions = ner.predict(X, mask)\n",
    "    test_preds.extend(predictions)\n",
    "test_preds = np.array(test_preds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "with open('new_test_preds.txt', 'w') as f:\n",
    "    for i in range(len(test_preds)):\n",
    "        for j in range(len(test_preds[i])):\n",
    "            f.write(labels_inv[test_preds[i][j]] + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION Report\n",
      "                 precision    recall  f1_score  true_entities  pred_entities\n",
      "Reagent           0.868080  0.830314  0.848777         6459.0         6178.0\n",
      "Action            0.844101  0.844660  0.844381         4532.0         4535.0\n",
      "Modifier          0.544426  0.579536  0.561433         2326.0         2476.0\n",
      "Amount            0.929877  0.868142  0.897949         2169.0         2025.0\n",
      "Location          0.774014  0.765500  0.769734         2000.0         1978.0\n",
      "Time              0.940197  0.899705  0.919506         1695.0         1622.0\n",
      "Method            0.616082  0.501845  0.553127         1084.0          883.0\n",
      "Temperature       0.947053  0.919496  0.933071         1031.0         1001.0\n",
      "Concentration     0.822835  0.846154  0.834331          988.0         1016.0\n",
      "Device            0.721788  0.727477  0.724621          888.0          895.0\n",
      "Measure-Type      0.551802  0.656836  0.599755          373.0          444.0\n",
      "Speed             0.916409  0.907975  0.912173          326.0          323.0\n",
      "Generic-Measure   0.600000  0.120401  0.200557          299.0           60.0\n",
      "Numerical         0.397388  0.712375  0.510180          299.0          536.0\n",
      "Size              0.680952  0.593361  0.634146          241.0          210.0\n",
      "pH                0.872881  0.774436  0.820717          133.0          118.0\n",
      "Seal              0.808081  0.701754  0.751174          114.0           99.0\n",
      "Mention           0.700000  0.474576  0.565657           59.0           40.0\n",
      "micro_avg         0.801833  0.783339  0.792478        25016.0        24439.0\n",
      "macro_avg         0.751998  0.706919  0.715627        25016.0        24439.0\n",
      "weighted_avg      0.806359  0.783339  0.791674        25016.0        24439.0\n",
      "\n",
      "F scores\n",
      "Micro-F1 = 0.7924780103124052\n",
      "Macro-F1 = 0.7156271057729451\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py Data/test.txt new_test_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
