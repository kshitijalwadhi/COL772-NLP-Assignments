{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/labels.json') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the labels\n",
    "labels_inv = {v: k for k, v in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.readlines()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file('Data/train.txt')\n",
    "val_data = read_file('Data/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "emb_dim = 50\n",
    "with open('glove.6B/glove.6B.50d.txt','r') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    embeddings[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_KEY = \"<numeric>\"\n",
    "UNK_KEY = \"<unk>\"\n",
    "\n",
    "ADDITIONAL_KEYS = [NUMERIC_KEY, UNK_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ADDITIONAL_KEYS:\n",
    "    embeddings[k] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_keys = list(embeddings.keys())\n",
    "vocab_keys = []\n",
    "vocab_keys.append(\"<unk>\")\n",
    "vocab_keys.append(\"<pad>\")\n",
    "vocab_keys.append(\"<numeric>\")\n",
    "vocab = {k: v for v, k in enumerate(vocab_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_vocab(data):\n",
    "    vocab = {}\n",
    "    num_words = 0\n",
    "    for line in data:\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = build_train_vocab(train_data)\n",
    "# extend the vocab with the train_vocab\n",
    "idx = len(vocab)\n",
    "for word in train_vocab:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quantity(word):\n",
    "    # check if the word is a quantity\n",
    "    if re.match(r'^\\d+\\.?\\d*[a-zA-Z]*$', word):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantity_vector(word):\n",
    "    num = re.findall(r'\\d+\\.?\\d*', word)\n",
    "    if len(num) > 0:\n",
    "        num = float(num[0])\n",
    "    else:\n",
    "        num = 0\n",
    "    unit = re.findall(r'[a-zA-Z]+', word)\n",
    "    if len(unit) > 0:\n",
    "        unit = unit[0]\n",
    "    else:\n",
    "        unit = \"\"\n",
    "    if unit in embeddings:\n",
    "        return embeddings[\"<numeric>\"] + embeddings[unit]\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    word = word.replace(\"~\",\"\")\n",
    "    temp = word.replace(\",\", \"\")\n",
    "    temp = temp.replace(\"-\", \"\")\n",
    "    if temp.replace(\".\", \"\", 1).isdigit():\n",
    "        return embeddings[NUMERIC_KEY]\n",
    "    elif word in embeddings:\n",
    "        return embeddings[word]\n",
    "    elif check_if_quantity(word):\n",
    "        return get_quantity_vector(word)\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab)\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    weights_matrix[i] = get_vector(word)\n",
    "    # if word in embeddings:\n",
    "    #     weights_matrix[i] = embeddings[word]\n",
    "    # else:\n",
    "    #     weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = torch.from_numpy(weights_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    sent_labels = []\n",
    "    all_labels = []\n",
    "    sent_idx = []\n",
    "    all_idx = []\n",
    "    for line in (data):\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            tag = split_line[1]\n",
    "            tag = tag.replace(\"\\n\", \"\")\n",
    "            word = word.lower()\n",
    "            if word in vocab:\n",
    "                sent_idx.append(vocab[word])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<unk>\"])\n",
    "            tag_idx = labels[tag]\n",
    "            sent_labels.append(tag_idx)\n",
    "        elif line==\"\\n\":\n",
    "            sent_idx = np.array(sent_idx)\n",
    "            sent_labels = np.array(sent_labels)\n",
    "            all_idx.append(sent_idx)\n",
    "            all_labels.append(sent_labels)\n",
    "            sent_idx = []\n",
    "            sent_labels = []\n",
    "        else:\n",
    "            print(line)\n",
    "    return np.asarray(all_idx, dtype=object), np.asarray(all_labels, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = get_data(train_data)\n",
    "valX, valY = get_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = []\n",
    "valData = []\n",
    "for i in range(len(trainX)):\n",
    "    trainData.append((trainX[i], trainY[i]))\n",
    "for i in range(len(valX)):\n",
    "    valData.append((valX[i], valY[i]))\n",
    "trainData = np.array(trainData, dtype=object)\n",
    "valData = np.array(valData, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    \n",
    "    batch_size = len(data)\n",
    "    \n",
    "    max_len = -1\n",
    "    for i in range(batch_size):\n",
    "        if len(data[i][0]) > max_len:\n",
    "            max_len = len(data[i][0])\n",
    "    \n",
    "    seq_lengths = []\n",
    "    for i in range(batch_size):\n",
    "        seq_lengths.append(len(data[i][0]))\n",
    "    \n",
    "    padded_data = []\n",
    "    padded_labels = []\n",
    "    mask = []\n",
    "    for i in range(batch_size):\n",
    "        padded_data.append(np.pad(data[i][0], (0, max_len-len(data[i][0])), 'constant', constant_values=(vocab[\"<pad>\"])))\n",
    "        padded_labels.append(np.pad(data[i][1], (0, max_len-len(data[i][1])), 'constant', constant_values=[\"37\"]))\n",
    "        mask.append(np.pad(np.ones(len(data[i][0])), (0, max_len-len(data[i][0])), 'constant', constant_values=0).astype(bool))\n",
    "    \n",
    "    padded_data = torch.from_numpy(np.array(padded_data))\n",
    "    padded_labels = torch.from_numpy(np.array(padded_labels))\n",
    "    mask = torch.from_numpy(np.array(mask))\n",
    "\n",
    "    return [padded_data, padded_labels, seq_lengths, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainDataLoader:\n",
    "    X, y, seq_lens, mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_dim, tagset_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze=False)\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout_layer   = nn.Dropout(p=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_dim, tagset_size):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze=False)\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout_layer = nn.Dropout(p=0.3)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence, labels, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return -self.crf(emissions, labels, mask=mask)\n",
    "\n",
    "    def predict(self, sentence, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        scores = self.hidden2tag(lstm_out)\n",
    "        return self.crf.decode(scores, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        X, y, seq_lens, mask = batch\n",
    "        loss = model(X, y, mask)\n",
    "        predictions = model.predict(X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(preds, gold):\n",
    "    flatten_preds = []\n",
    "    flatten_gold = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            flatten_preds.append(preds[i][j])\n",
    "            flatten_gold.append(gold[i][j])\n",
    "    idx = np.where(np.array(flatten_gold) != 0)[0]\n",
    "    micro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='micro')\n",
    "    macro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='macro')\n",
    "    return micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=37)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in (range(epochs)):\n",
    "        print(\"Training Epoch {}\".format(epoch))\n",
    "        training_loss = train_one_epoch(model, trainDataLoader, optimizer, loss_function)\n",
    "        print(\"Training Loss: {}\".format(training_loss))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        train_preds = []\n",
    "        for batch in trainDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            train_preds.extend(predictions)\n",
    "        train_preds = np.array(train_preds, dtype=object)\n",
    "\n",
    "        train_micro_f1, train_macro_f1 = get_scores(train_preds, trainY)\n",
    "\n",
    "        val_preds = []\n",
    "        for batch in valDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            val_preds.extend(predictions)\n",
    "        val_preds = np.array(val_preds, dtype=object)\n",
    "\n",
    "        val_micro_f1, val_macro_f1 = get_scores(val_preds, valY)\n",
    "\n",
    "        print(\"Training Micro F1: {}\".format(train_micro_f1))\n",
    "        print(\"Training Macro F1: {}\".format(train_macro_f1))\n",
    "        print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "        print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "\n",
    "        train_f1 = (train_micro_f1 + train_macro_f1) / 2\n",
    "        val_f1 = (val_micro_f1 + val_macro_f1) / 2\n",
    "\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            print(\"New Best Model at Epoch {}\".format(epoch))\n",
    "            print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "            print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        \n",
    "        if epoch>=best_epoch + 3:\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    return model, train_f1s, val_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = BiLSTMCRF(weights_matrix, 256, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:34<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 778.0048285590278\n",
      "Training Micro F1: 0.5147155515200852\n",
      "Training Macro F1: 0.2786839400521984\n",
      "Validation Micro F1: 0.469359808284834\n",
      "Validation Macro F1: 0.25728418251663\n",
      "New Best Model at Epoch 0\n",
      "Validation Micro F1: 0.469359808284834\n",
      "Validation Macro F1: 0.25728418251663\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:35<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 504.4840157063802\n",
      "Training Micro F1: 0.6020695701180375\n",
      "Training Macro F1: 0.3917065572901869\n",
      "Validation Micro F1: 0.5386609282535335\n",
      "Validation Macro F1: 0.3561231410148542\n",
      "New Best Model at Epoch 1\n",
      "Validation Micro F1: 0.5386609282535335\n",
      "Validation Macro F1: 0.3561231410148542\n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 418.88360026041664\n",
      "Training Micro F1: 0.6553899621152822\n",
      "Training Macro F1: 0.47286465359222857\n",
      "Validation Micro F1: 0.577297403042011\n",
      "Validation Macro F1: 0.4230122446541476\n",
      "New Best Model at Epoch 2\n",
      "Validation Micro F1: 0.577297403042011\n",
      "Validation Macro F1: 0.4230122446541476\n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 361.76423597547745\n",
      "Training Micro F1: 0.6858855944143524\n",
      "Training Macro F1: 0.5234355206728373\n",
      "Validation Micro F1: 0.5992566146622976\n",
      "Validation Macro F1: 0.46791468300553307\n",
      "New Best Model at Epoch 3\n",
      "Validation Micro F1: 0.5992566146622976\n",
      "Validation Macro F1: 0.46791468300553307\n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 319.81729790581596\n",
      "Training Micro F1: 0.7119665612573969\n",
      "Training Macro F1: 0.5727905688176599\n",
      "Validation Micro F1: 0.6136841590453367\n",
      "Validation Macro F1: 0.5022277434466136\n",
      "New Best Model at Epoch 4\n",
      "Validation Micro F1: 0.6136841590453367\n",
      "Validation Macro F1: 0.5022277434466136\n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:34<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 286.6393647596571\n",
      "Training Micro F1: 0.7354488243213627\n",
      "Training Macro F1: 0.6133023665052525\n",
      "Validation Micro F1: 0.6263510539443439\n",
      "Validation Macro F1: 0.5317327344612656\n",
      "New Best Model at Epoch 5\n",
      "Validation Micro F1: 0.6263510539443439\n",
      "Validation Macro F1: 0.5317327344612656\n",
      "Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:34<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 258.48229905870227\n",
      "Training Micro F1: 0.7563793481323774\n",
      "Training Macro F1: 0.6472196189698626\n",
      "Validation Micro F1: 0.6340783489020394\n",
      "Validation Macro F1: 0.5500900049461157\n",
      "New Best Model at Epoch 6\n",
      "Validation Micro F1: 0.6340783489020394\n",
      "Validation Macro F1: 0.5500900049461157\n",
      "Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:34<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 235.02923075358072\n",
      "Training Micro F1: 0.774413726165503\n",
      "Training Macro F1: 0.6773496685954213\n",
      "Validation Micro F1: 0.6348608597838313\n",
      "Validation Macro F1: 0.5549235972223823\n",
      "New Best Model at Epoch 7\n",
      "Validation Micro F1: 0.6348608597838313\n",
      "Validation Macro F1: 0.5549235972223823\n",
      "Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:35<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 214.86238762749565\n",
      "Training Micro F1: 0.7912113716772597\n",
      "Training Macro F1: 0.7031194958052785\n",
      "Validation Micro F1: 0.6378930894507752\n",
      "Validation Macro F1: 0.5620776782414733\n",
      "New Best Model at Epoch 8\n",
      "Validation Micro F1: 0.6378930894507752\n",
      "Validation Macro F1: 0.5620776782414733\n",
      "Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 196.3770221625434\n",
      "Training Micro F1: 0.8084786624502959\n",
      "Training Macro F1: 0.7258235037678938\n",
      "Validation Micro F1: 0.6380398102411111\n",
      "Validation Macro F1: 0.5655844460530236\n",
      "New Best Model at Epoch 9\n",
      "Validation Micro F1: 0.6380398102411111\n",
      "Validation Macro F1: 0.5655844460530236\n",
      "Training Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:35<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 179.55515228271486\n",
      "Training Micro F1: 0.8262782178527819\n",
      "Training Macro F1: 0.747272839104776\n",
      "Validation Micro F1: 0.6371105785689832\n",
      "Validation Macro F1: 0.5640966284028532\n",
      "Training Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:35<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 162.69178327772352\n",
      "Training Micro F1: 0.8455962929334043\n",
      "Training Macro F1: 0.7721255772769287\n",
      "Validation Micro F1: 0.6373062062894312\n",
      "Validation Macro F1: 0.5594101945032083\n",
      "Training Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 145.81724202473958\n",
      "Training Micro F1: 0.8628948933905257\n",
      "Training Macro F1: 0.7958880417243573\n",
      "Validation Micro F1: 0.6349586736440553\n",
      "Validation Macro F1: 0.5543933326860897\n"
     ]
    }
   ],
   "source": [
    "ner, train_f1s, val_f1s = train_model(ner, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model\n",
    "ner.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMCRF(\n",
       "  (embedding): Embedding(7400, 50)\n",
       "  (lstm): LSTM(50, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=38, bias=True)\n",
       "  (crf): CRF(num_tags=38)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation predictions using valDataloader\n",
    "val_preds = []\n",
    "for batch in valDataLoader:\n",
    "    X, y, seq_lens, mask = batch\n",
    "    predictions = ner.predict(X, mask)\n",
    "    val_preds.extend(predictions)\n",
    "val_preds = np.array(val_preds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_val_preds = []\n",
    "flatten_valY = []\n",
    "for i in range(len(val_preds)):\n",
    "    for j in range(len(val_preds[i])):\n",
    "        flatten_val_preds.append(val_preds[i][j])\n",
    "        flatten_valY.append(valY[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7689363693934305"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(flatten_valY, flatten_val_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5756355319572172"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(flatten_valY, flatten_val_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6380398102411111"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx where flatten_valY is not 0\n",
    "idx = np.where(np.array(flatten_valY) != 0)[0]\n",
    "f1_score(np.array(flatten_valY)[idx], np.array(flatten_val_preds)[idx], average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5655844460530236"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.array(flatten_valY)[idx], np.array(flatten_val_preds)[idx], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6, 14, 15, 2, 3, 0, 0, 22, 0, 0, 26, 0]),\n",
       "       list([6, 2, 0, 0, 0, 6, 14, 15, 0, 16, 17, 2, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 0, 0]),\n",
       "       list([6, 14, 15, 12, 27, 0, 2, 3, 0]), ...,\n",
       "       list([6, 0, 22, 0, 11, 12, 0, 6, 2, 0, 0, 22, 0]),\n",
       "       list([6, 4, 0, 1, 0, 6, 0, 22, 0]),\n",
       "       list([6, 0, 18, 19, 0, 11, 12, 0, 0, 0, 4, 6, 0, 6, 0, 2, 0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "with open('val_preds.txt', 'w') as f:\n",
    "    for i in range(len(val_preds)):\n",
    "        for j in range(len(val_preds[i])):\n",
    "            f.write(labels_inv[val_preds[i][j]] + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
