{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/labels.json') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.readlines()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file('Data/train.txt')\n",
    "val_data = read_file('Data/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "emb_dim = 50\n",
    "with open('glove.6B/glove.6B.50d.txt','r') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    embeddings[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_keys = list(embeddings.keys())\n",
    "vocab_keys.append(\"<unk>\")\n",
    "vocab_keys.append(\"<pad>\")\n",
    "vocab = {k: v for v, k in enumerate(vocab_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab)\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = torch.from_numpy(weights_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    sent_labels = []\n",
    "    all_labels = []\n",
    "    sent_idx = []\n",
    "    all_idx = []\n",
    "    for line in (data):\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            tag = split_line[1]\n",
    "            tag = tag.replace(\"\\n\", \"\")\n",
    "            word = word.lower()\n",
    "            if word in vocab:\n",
    "                sent_idx.append(vocab[word])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<unk>\"])\n",
    "            tag_idx = labels[tag]\n",
    "            sent_labels.append(tag_idx)\n",
    "        elif line==\"\\n\":\n",
    "            sent_idx = np.array(sent_idx)\n",
    "            sent_labels = np.array(sent_labels)\n",
    "            all_idx.append(sent_idx)\n",
    "            all_labels.append(sent_labels)\n",
    "            sent_idx = []\n",
    "            sent_labels = []\n",
    "    return np.asarray(all_idx, dtype=object), np.asarray(all_labels, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = get_data(train_data)\n",
    "valX, valY = get_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = []\n",
    "valData = []\n",
    "for i in range(len(trainX)):\n",
    "    trainData.append((trainX[i], trainY[i]))\n",
    "for i in range(len(valX)):\n",
    "    valData.append((valX[i], valY[i]))\n",
    "trainData = np.array(trainData, dtype=object)\n",
    "valData = np.array(valData, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    \n",
    "    batch_size = len(data)\n",
    "    \n",
    "    max_len = -1\n",
    "    for i in range(batch_size):\n",
    "        if len(data[i][0]) > max_len:\n",
    "            max_len = len(data[i][0])\n",
    "    \n",
    "    padded_data = []\n",
    "    padded_labels = []\n",
    "    for i in range(batch_size):\n",
    "        padded_data.append(np.pad(data[i][0], (0, max_len-len(data[i][0])), 'constant', constant_values=(vocab[\"<pad>\"])))\n",
    "        padded_labels.append(np.pad(data[i][1], (0, max_len-len(data[i][1])), 'constant', constant_values=[\"37\"]))\n",
    "    \n",
    "    padded_data = torch.from_numpy(np.array(padded_data))\n",
    "    padded_labels = torch.from_numpy(np.array(padded_labels))\n",
    "\n",
    "    return [padded_data, padded_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_dim, tagset_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_dim),\n",
    "                torch.zeros(2, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        X, y = batch\n",
    "        predictions = model(X)\n",
    "        predictions = predictions.permute(0, 2, 1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in (range(epochs)):\n",
    "        print(\"Training Epoch {}\".format(epoch))\n",
    "        training_loss = train_one_epoch(model, trainDataLoader, optimizer, loss_function)\n",
    "        print(\"Training Loss: {}\".format(training_loss))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = BiLSTM(weights_matrix, 64, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 457.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6430135454071895\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 474.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6213050679365795\n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 471.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6108315796322294\n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 475.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5932151340113747\n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 476.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5796316646205053\n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 467.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.569438490205341\n",
      "Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 459.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5585711288452146\n",
      "Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 459.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5481018348534903\n",
      "Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 463.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5391218022505444\n",
      "Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:03<00:00, 479.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.531314436859555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(ner, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(valX)):\n",
    "    inp = torch.tensor(valX[i])\n",
    "    out = ner(inp)\n",
    "    # take the argmax of the output\n",
    "    out = out.detach().numpy()\n",
    "    out = np.argmax(out, axis=1)\n",
    "    \n",
    "    gold = valY[i]\n",
    "    correct += np.sum(out == gold)\n",
    "    total += len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.52465651587324"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
