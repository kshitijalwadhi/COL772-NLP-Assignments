{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/labels.json') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the labels\n",
    "labels_inv = {v: k for k, v in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.readlines()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file('Data/train.txt')\n",
    "val_data = read_file('Data/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "emb_dim = 50\n",
    "with open('glove.6B/glove.6B.50d.txt','r') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    embeddings[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_KEY = \"<numeric>\"\n",
    "UNK_KEY = \"<unk>\"\n",
    "\n",
    "ADDITIONAL_KEYS = [NUMERIC_KEY, UNK_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ADDITIONAL_KEYS:\n",
    "    embeddings[k] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_keys = list(embeddings.keys())\n",
    "vocab_keys = []\n",
    "vocab_keys.append(\"<unk>\")\n",
    "vocab_keys.append(\"<pad>\")\n",
    "vocab_keys.append(\"<numeric>\")\n",
    "vocab = {k: v for v, k in enumerate(vocab_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_vocab(data):\n",
    "    vocab = {}\n",
    "    num_words = 0\n",
    "    for line in data:\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = build_train_vocab(train_data)\n",
    "# extend the vocab with the train_vocab\n",
    "idx = len(vocab)\n",
    "for word in train_vocab:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7400"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump vocab\n",
    "with open('vocab.json', 'w') as fp:\n",
    "    json.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quantity(word):\n",
    "    # check if the word is a quantity\n",
    "    if re.match(r'^\\d+\\.?\\d*[a-zA-Z]*$', word):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantity_vector(word):\n",
    "    num = re.findall(r'\\d+\\.?\\d*', word)\n",
    "    if len(num) > 0:\n",
    "        num = float(num[0])\n",
    "    else:\n",
    "        num = 0\n",
    "    unit = re.findall(r'[a-zA-Z]+', word)\n",
    "    if len(unit) > 0:\n",
    "        unit = unit[0]\n",
    "    else:\n",
    "        unit = \"\"\n",
    "    if unit in embeddings:\n",
    "        return embeddings[\"<numeric>\"] + embeddings[unit]\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    word = word.replace(\"~\",\"\")\n",
    "    temp = word.replace(\",\", \"\")\n",
    "    temp = temp.replace(\"-\", \"\")\n",
    "    if temp.replace(\".\", \"\", 1).isdigit():\n",
    "        return embeddings[NUMERIC_KEY]\n",
    "    elif word in embeddings:\n",
    "        return embeddings[word]\n",
    "    # elif check_if_quantity(word):\n",
    "    #     return get_quantity_vector(word)\n",
    "    else:\n",
    "        return np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab)\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    weights_matrix[i] = get_vector(word)\n",
    "    # if word in embeddings:\n",
    "    #     weights_matrix[i] = embeddings[word]\n",
    "    # else:\n",
    "    #     weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = torch.from_numpy(weights_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    sent_labels = []\n",
    "    all_labels = []\n",
    "    sent_idx = []\n",
    "    all_idx = []\n",
    "    for line in (data):\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            tag = split_line[1]\n",
    "            tag = tag.replace(\"\\n\", \"\")\n",
    "            word = word.lower()\n",
    "            if word in vocab:\n",
    "                sent_idx.append(vocab[word])\n",
    "            else:\n",
    "                sent_idx.append(vocab[\"<unk>\"])\n",
    "            tag_idx = labels[tag]\n",
    "            sent_labels.append(tag_idx)\n",
    "        elif line==\"\\n\":\n",
    "            sent_idx = np.array(sent_idx)\n",
    "            sent_labels = np.array(sent_labels)\n",
    "            all_idx.append(sent_idx)\n",
    "            all_labels.append(sent_labels)\n",
    "            sent_idx = []\n",
    "            sent_labels = []\n",
    "        else:\n",
    "            print(line)\n",
    "    return np.asarray(all_idx, dtype=object), np.asarray(all_labels, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = get_data(train_data)\n",
    "valX, valY = get_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = []\n",
    "valData = []\n",
    "for i in range(len(trainX)):\n",
    "    trainData.append((trainX[i], trainY[i]))\n",
    "for i in range(len(valX)):\n",
    "    valData.append((valX[i], valY[i]))\n",
    "trainData = np.array(trainData, dtype=object)\n",
    "valData = np.array(valData, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    \n",
    "    batch_size = len(data)\n",
    "    \n",
    "    max_len = -1\n",
    "    for i in range(batch_size):\n",
    "        if len(data[i][0]) > max_len:\n",
    "            max_len = len(data[i][0])\n",
    "    \n",
    "    seq_lengths = []\n",
    "    for i in range(batch_size):\n",
    "        seq_lengths.append(len(data[i][0]))\n",
    "    \n",
    "    padded_data = []\n",
    "    padded_labels = []\n",
    "    mask = []\n",
    "    for i in range(batch_size):\n",
    "        padded_data.append(np.pad(data[i][0], (0, max_len-len(data[i][0])), 'constant', constant_values=(vocab[\"<pad>\"])))\n",
    "        padded_labels.append(np.pad(data[i][1], (0, max_len-len(data[i][1])), 'constant', constant_values=[\"37\"]))\n",
    "        mask.append(np.pad(np.ones(len(data[i][0])), (0, max_len-len(data[i][0])), 'constant', constant_values=0).astype(bool))\n",
    "    \n",
    "    padded_data = torch.from_numpy(np.array(padded_data))\n",
    "    padded_labels = torch.from_numpy(np.array(padded_labels))\n",
    "    mask = torch.from_numpy(np.array(mask))\n",
    "\n",
    "    return [padded_data, padded_labels, seq_lengths, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainDataLoader:\n",
    "    X, y, seq_lens, mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_dim, tagset_size):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze=False)\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        # self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence, labels, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return -self.crf(emissions, labels, mask=mask)\n",
    "\n",
    "    def predict(self, sentence, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        scores = self.hidden2tag(lstm_out)\n",
    "        return self.crf.decode(scores, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        X, y, seq_lens, mask = batch\n",
    "        loss = model(X, y, mask)\n",
    "        predictions = model.predict(X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(preds, gold):\n",
    "    flatten_preds = []\n",
    "    flatten_gold = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            flatten_preds.append(preds[i][j])\n",
    "            flatten_gold.append(gold[i][j])\n",
    "    idx = np.where(np.array(flatten_gold) != 0)[0]\n",
    "    micro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='micro')\n",
    "    macro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='macro')\n",
    "    return micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=37)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in (range(epochs)):\n",
    "        print(\"Training Epoch {}\".format(epoch))\n",
    "        training_loss = train_one_epoch(model, trainDataLoader, optimizer, loss_function)\n",
    "        print(\"Training Loss: {}\".format(training_loss))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        train_preds = []\n",
    "        for batch in trainDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            train_preds.extend(predictions)\n",
    "        train_preds = np.array(train_preds, dtype=object)\n",
    "\n",
    "        train_micro_f1, train_macro_f1 = get_scores(train_preds, trainY)\n",
    "\n",
    "        val_preds = []\n",
    "        for batch in valDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            val_preds.extend(predictions)\n",
    "        val_preds = np.array(val_preds, dtype=object)\n",
    "\n",
    "        val_micro_f1, val_macro_f1 = get_scores(val_preds, valY)\n",
    "\n",
    "        print(\"Training Micro F1: {}\".format(train_micro_f1))\n",
    "        print(\"Training Macro F1: {}\".format(train_macro_f1))\n",
    "        print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "        print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "\n",
    "        train_f1 = (train_micro_f1 + train_macro_f1) / 2\n",
    "        val_f1 = (val_micro_f1 + val_macro_f1) / 2\n",
    "\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            print(\"New Best Model at Epoch {}\".format(epoch))\n",
    "            print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "            print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        \n",
    "        if epoch>=best_epoch + 3:\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    return model, train_f1s, val_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = BiLSTMCRF(weights_matrix, 256, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:35<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 700.6398346625434\n",
      "Training Micro F1: 0.5683490403581828\n",
      "Training Macro F1: 0.33070636444002394\n",
      "Validation Micro F1: 0.4954272020345283\n",
      "Validation Macro F1: 0.3003557460059168\n",
      "New Best Model at Epoch 0\n",
      "Validation Micro F1: 0.4954272020345283\n",
      "Validation Macro F1: 0.3003557460059168\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 449.2090400526259\n",
      "Training Micro F1: 0.6529791164407152\n",
      "Training Macro F1: 0.47871753764206376\n",
      "Validation Micro F1: 0.5592507458306842\n",
      "Validation Macro F1: 0.4269260284001654\n",
      "New Best Model at Epoch 1\n",
      "Validation Micro F1: 0.5592507458306842\n",
      "Validation Macro F1: 0.4269260284001654\n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 370.2796192762587\n",
      "Training Micro F1: 0.7004132878299258\n",
      "Training Macro F1: 0.541361979989149\n",
      "Validation Micro F1: 0.5840465593974666\n",
      "Validation Macro F1: 0.47147133233288335\n",
      "New Best Model at Epoch 2\n",
      "Validation Micro F1: 0.5840465593974666\n",
      "Validation Macro F1: 0.47147133233288335\n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 317.7520138888889\n",
      "Training Micro F1: 0.7331788722251794\n",
      "Training Macro F1: 0.6052369469656443\n",
      "Validation Micro F1: 0.5987186384310657\n",
      "Validation Macro F1: 0.520977014940782\n",
      "New Best Model at Epoch 3\n",
      "Validation Micro F1: 0.5987186384310657\n",
      "Validation Macro F1: 0.520977014940782\n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:38<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 278.20322068956165\n",
      "Training Micro F1: 0.7596981746454178\n",
      "Training Macro F1: 0.6469984366848226\n",
      "Validation Micro F1: 0.6078642343620091\n",
      "Validation Macro F1: 0.5428483985564891\n",
      "New Best Model at Epoch 4\n",
      "Validation Micro F1: 0.6078642343620091\n",
      "Validation Macro F1: 0.5428483985564891\n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:38<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 247.06235643174912\n",
      "Training Micro F1: 0.7818184664516736\n",
      "Training Macro F1: 0.6795716418920611\n",
      "Validation Micro F1: 0.6135374382550007\n",
      "Validation Macro F1: 0.5505621362916454\n",
      "New Best Model at Epoch 5\n",
      "Validation Micro F1: 0.6135374382550007\n",
      "Validation Macro F1: 0.5505621362916454\n",
      "Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 220.76384677463108\n",
      "Training Micro F1: 0.8021697611071105\n",
      "Training Macro F1: 0.7112822149883017\n",
      "Validation Micro F1: 0.6186237589866485\n",
      "Validation Macro F1: 0.5636699360371269\n",
      "New Best Model at Epoch 6\n",
      "Validation Micro F1: 0.6186237589866485\n",
      "Validation Macro F1: 0.5636699360371269\n",
      "Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:36<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 199.53359419080945\n",
      "Training Micro F1: 0.819155264723379\n",
      "Training Macro F1: 0.737946391774091\n",
      "Validation Micro F1: 0.6182325035457524\n",
      "Validation Macro F1: 0.5691037896672797\n",
      "New Best Model at Epoch 7\n",
      "Validation Micro F1: 0.6182325035457524\n",
      "Validation Macro F1: 0.5691037896672797\n",
      "Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 179.40003217909072\n",
      "Training Micro F1: 0.8390525689595794\n",
      "Training Macro F1: 0.7625995983913573\n",
      "Validation Micro F1: 0.6184281312662004\n",
      "Validation Macro F1: 0.5685909140715285\n",
      "Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 161.19162170410155\n",
      "Training Micro F1: 0.856069382259933\n",
      "Training Macro F1: 0.7846236403116618\n",
      "Validation Micro F1: 0.6178412481048564\n",
      "Validation Macro F1: 0.5695699345564489\n",
      "New Best Model at Epoch 9\n",
      "Validation Micro F1: 0.6178412481048564\n",
      "Validation Macro F1: 0.5695699345564489\n",
      "Training Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:37<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 144.00107245551214\n",
      "Training Micro F1: 0.8758884122859201\n",
      "Training Macro F1: 0.8133150892297829\n",
      "Validation Micro F1: 0.6156404362498166\n",
      "Validation Macro F1: 0.5638314051110899\n",
      "Training Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 77/225 [00:13<00:25,  5.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ner, train_f1s, val_f1s \u001b[39m=\u001b[39m train_model(ner, \u001b[39m30\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [62], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m (\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining Epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m---> 13\u001b[0m     training_loss \u001b[39m=\u001b[39m train_one_epoch(model, trainDataLoader, optimizer, loss_function)\n\u001b[1;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining Loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(training_loss))\n\u001b[1;32m     16\u001b[0m     model\u001b[39m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn [60], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m X, y, seq_lens, mask \u001b[39m=\u001b[39m batch\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m model(X, y, mask)\n\u001b[0;32m----> 9\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X, mask)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn [59], line 23\u001b[0m, in \u001b[0;36mBiLSTMCRF.predict\u001b[0;34m(self, sentence, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, sentence, mask):\n\u001b[1;32m     22\u001b[0m     embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(sentence)\n\u001b[0;32m---> 23\u001b[0m     lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embeds)\n\u001b[1;32m     24\u001b[0m     lstm_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_layer(lstm_out)\n\u001b[1;32m     25\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden2tag(lstm_out)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ner, train_f1s, val_f1s = train_model(ner, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model\n",
    "ner.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ner, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMCRF(\n",
       "  (embedding): Embedding(7400, 200)\n",
       "  (lstm): LSTM(200, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=38, bias=True)\n",
       "  (crf): CRF(num_tags=38)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ner.state_dict(), \"kshitij.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation predictions using valDataloader\n",
    "val_preds = []\n",
    "for batch in valDataLoader:\n",
    "    X, y, seq_lens, mask = batch\n",
    "    predictions = ner.predict(X, mask)\n",
    "    val_preds.extend(predictions)\n",
    "val_preds = np.array(val_preds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_val_preds = []\n",
    "flatten_valY = []\n",
    "for i in range(len(val_preds)):\n",
    "    for j in range(len(val_preds[i])):\n",
    "        flatten_val_preds.append(val_preds[i][j])\n",
    "        flatten_valY.append(valY[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7604834934882568"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(flatten_valY, flatten_val_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5791153088548913"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(flatten_valY, flatten_val_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178412481048564"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx where flatten_valY is not 0\n",
    "idx = np.where(np.array(flatten_valY) != 0)[0]\n",
    "f1_score(np.array(flatten_valY)[idx], np.array(flatten_val_preds)[idx], average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5695699345564489"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.array(flatten_valY)[idx], np.array(flatten_val_preds)[idx], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6, 14, 15, 2, 3, 0, 0, 22, 0, 0, 26, 0]),\n",
       "       list([6, 2, 0, 0, 0, 6, 14, 15, 0, 16, 17, 2, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 0, 0]),\n",
       "       list([6, 14, 15, 12, 27, 0, 2, 3, 0]), ...,\n",
       "       list([6, 0, 22, 0, 11, 12, 0, 6, 2, 0, 0, 22, 0]),\n",
       "       list([6, 4, 0, 1, 0, 6, 0, 22, 0]),\n",
       "       list([6, 0, 18, 19, 0, 11, 12, 0, 0, 0, 4, 6, 0, 6, 0, 2, 0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "with open('val_preds.txt', 'w') as f:\n",
    "    for i in range(len(val_preds)):\n",
    "        for j in range(len(val_preds[i])):\n",
    "            f.write(labels_inv[val_preds[i][j]] + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION Report\n",
      "                 precision    recall  f1_score  true_entities  pred_entities\n",
      "Reagent           0.808605  0.757997  0.782484         5033.0         4718.0\n",
      "Action            0.838700  0.794231  0.815860         3640.0         3447.0\n",
      "Modifier          0.584369  0.329329  0.421255         1998.0         1126.0\n",
      "Location          0.779203  0.580191  0.665130         1989.0         1481.0\n",
      "Amount            0.876660  0.806752  0.840255         1718.0         1581.0\n",
      "Time              0.902857  0.862602  0.882271         1099.0         1050.0\n",
      "Device            0.563830  0.528239  0.545455          903.0          846.0\n",
      "Method            0.536210  0.385809  0.448743          902.0          649.0\n",
      "Concentration     0.784411  0.668079  0.721587          708.0          603.0\n",
      "Temperature       0.907716  0.895522  0.901578          670.0          661.0\n",
      "Measure-Type      0.563187  0.476744  0.516373          430.0          364.0\n",
      "Generic-Measure   0.503759  0.223333  0.309469          300.0          133.0\n",
      "Speed             0.887967  0.715719  0.792593          299.0          241.0\n",
      "Numerical         0.512500  0.473077  0.492000          260.0          240.0\n",
      "Size              0.739130  0.461538  0.568245          221.0          138.0\n",
      "Seal              0.766667  0.547619  0.638889          126.0           90.0\n",
      "Mention           0.725806  0.523256  0.608108           86.0           62.0\n",
      "pH                0.947368  0.830769  0.885246           65.0           57.0\n",
      "micro_avg         0.779379  0.666553  0.718564        20447.0        17487.0\n",
      "macro_avg         0.734942  0.603378  0.657530        20447.0        17487.0\n",
      "weighted_avg      0.766404  0.666553  0.708843        20447.0        17487.0\n",
      "\n",
      "F scores\n",
      "Micro-F1 = 0.7185638213739654\n",
      "Macro-F1 = 0.6575298703887407\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py Data/dev.txt val_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7198"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2267"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
