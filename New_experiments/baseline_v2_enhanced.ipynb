{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/labels.json') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the labels\n",
    "labels_inv = {v: k for k, v in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.readlines()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file('Data/train.txt')\n",
    "val_data = read_file('Data/dev.txt')\n",
    "test_data = read_file(\"Data/test.txt\")\n",
    "test_data2 = read_file(\"Data/test_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "emb_dim = 50\n",
    "with open('glove.6B/glove.6B.50d.txt','r') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    embeddings[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_KEY = \"<numeric>\"\n",
    "UNK_KEY = \"<unk>\"\n",
    "PAD_KEY = \"<pad>\"\n",
    "CONC_KEY = \"<conc>\"\n",
    "TILDA_KEY = \"<tilda>\"\n",
    "TILDA_NUM_KEY = \"<til_num>\"\n",
    "SPEED_KEY = \"<speed>\"\n",
    "\n",
    "ADDITIONAL_KEYS = [NUMERIC_KEY, UNK_KEY, PAD_KEY, CONC_KEY, TILDA_KEY, TILDA_NUM_KEY, SPEED_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ADDITIONAL_KEYS:\n",
    "    embeddings[k] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_keys = []\n",
    "vocab_keys.append(NUMERIC_KEY)\n",
    "vocab_keys.append(UNK_KEY)\n",
    "vocab_keys.append(PAD_KEY)\n",
    "vocab_keys.append(CONC_KEY)\n",
    "vocab_keys.append(TILDA_KEY)\n",
    "vocab_keys.append(TILDA_NUM_KEY)\n",
    "vocab_keys.append(SPEED_KEY)\n",
    "vocab = {k: v for v, k in enumerate(vocab_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_vocab(data):\n",
    "    vocab = {}\n",
    "    num_words = 0\n",
    "    for line in data:\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data + val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = build_train_vocab(train_data)\n",
    "# extend the vocab with the train_vocab\n",
    "idx = len(vocab)\n",
    "for word in train_vocab:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7404"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump vocab\n",
    "with open('vocab.json', 'w') as fp:\n",
    "    json.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_conc(word):\n",
    "    # check if word is a concentration\n",
    "    if re.match(r'[a-zA-Z]*\\/[a-zA-Z]*', word):\n",
    "        return True\n",
    "    elif word == \"%\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numeric(word):\n",
    "    word = word.replace(\",\", \"\")\n",
    "    word = word.replace(\"-\", \"\", 1)\n",
    "    word = word.replace(\".\", \"\", 1)\n",
    "    if word.isdigit():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_speed(word):\n",
    "    if \"xg\" in word:\n",
    "        return True\n",
    "    elif \"rpm\" in word:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):       \n",
    "    if check_if_speed(word):\n",
    "        return embeddings[SPEED_KEY]\n",
    "    elif check_numeric(word):\n",
    "        return embeddings[NUMERIC_KEY]\n",
    "    elif word in embeddings:\n",
    "        return embeddings[word]\n",
    "    else:\n",
    "        return embeddings[UNK_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_inference(word, vocab):\n",
    "    if check_if_speed(word):\n",
    "        return vocab[SPEED_KEY]\n",
    "    elif check_numeric(word):\n",
    "        return vocab[NUMERIC_KEY]\n",
    "    elif word in vocab:\n",
    "        return vocab[word]\n",
    "    else:\n",
    "        return vocab[UNK_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab)\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    weights_matrix[i] = get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = torch.from_numpy(weights_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    sent_labels = []\n",
    "    all_labels = []\n",
    "    sent_idx = []\n",
    "    all_idx = []\n",
    "    for line in (data):\n",
    "        split_line = line.split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word = split_line[0]\n",
    "            tag = split_line[1]\n",
    "            tag = tag.replace(\"\\n\", \"\")\n",
    "            word = word.lower()\n",
    "            sent_idx.append(get_idx_inference(word, vocab))\n",
    "            tag_idx = labels[tag]\n",
    "            sent_labels.append(tag_idx)\n",
    "        elif line==\"\\n\":\n",
    "            sent_idx = np.array(sent_idx)\n",
    "            sent_labels = np.array(sent_labels)\n",
    "            all_idx.append(sent_idx)\n",
    "            all_labels.append(sent_labels)\n",
    "            sent_idx = []\n",
    "            sent_labels = []\n",
    "        else:\n",
    "            print(line)\n",
    "    return np.asarray(all_idx, dtype=object), np.asarray(all_labels, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = get_data(train_data)\n",
    "valX, valY = get_data(val_data)\n",
    "test1X, test1Y = get_data(test_data)\n",
    "test2X, test2Y = get_data(test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "test1X, test1Y = shuffle(test1X, test1Y, random_state=42)\n",
    "test2X, test2Y = shuffle(test2X, test2Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend trainX with test1X and test2X\n",
    "trainX = np.concatenate((trainX, test1X[:2000], test2X[:2000]), axis=0)\n",
    "trainY = np.concatenate((trainY, test1Y[:2000], test2Y[:2000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = test1X\n",
    "testY = test1Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = []\n",
    "valData = []\n",
    "testData = []\n",
    "for i in range(len(trainX)):\n",
    "    trainData.append((trainX[i], trainY[i]))\n",
    "for i in range(len(valX)):\n",
    "    valData.append((valX[i], valY[i]))\n",
    "for i in range(len(testX)):\n",
    "    testData.append((testX[i], testY[i]))\n",
    "trainData = np.array(trainData, dtype=object)\n",
    "valData = np.array(valData, dtype=object)\n",
    "testData = np.array(testData, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    \n",
    "    batch_size = len(data)\n",
    "    \n",
    "    max_len = -1\n",
    "    for i in range(batch_size):\n",
    "        if len(data[i][0]) > max_len:\n",
    "            max_len = len(data[i][0])\n",
    "    \n",
    "    seq_lengths = []\n",
    "    for i in range(batch_size):\n",
    "        seq_lengths.append(len(data[i][0]))\n",
    "    \n",
    "    padded_data = []\n",
    "    padded_labels = []\n",
    "    mask = []\n",
    "    for i in range(batch_size):\n",
    "        padded_data.append(np.pad(data[i][0], (0, max_len-len(data[i][0])), 'constant', constant_values=(vocab[\"<pad>\"])))\n",
    "        padded_labels.append(np.pad(data[i][1], (0, max_len-len(data[i][1])), 'constant', constant_values=[\"37\"]))\n",
    "        mask.append(np.pad(np.ones(len(data[i][0])), (0, max_len-len(data[i][0])), 'constant', constant_values=0).astype(bool))\n",
    "    \n",
    "    padded_data = torch.from_numpy(np.array(padded_data))\n",
    "    padded_labels = torch.from_numpy(np.array(padded_labels))\n",
    "    mask = torch.from_numpy(np.array(mask))\n",
    "\n",
    "    return [padded_data, padded_labels, seq_lengths, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainDataLoader:\n",
    "    X, y, seq_lens, mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_dim, tagset_size):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze=False)\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        # self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence, labels, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return -self.crf(emissions, labels, mask=mask)\n",
    "\n",
    "    def predict(self, sentence, mask):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        scores = self.hidden2tag(lstm_out)\n",
    "        return self.crf.decode(scores, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        X, y, seq_lens, mask = batch\n",
    "        loss = model(X, y, mask)\n",
    "        predictions = model.predict(X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(preds, gold):\n",
    "    flatten_preds = []\n",
    "    flatten_gold = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            flatten_preds.append(preds[i][j])\n",
    "            flatten_gold.append(gold[i][j])\n",
    "    idx = np.where(np.array(flatten_gold) != 0)[0]\n",
    "    micro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='micro')\n",
    "    macro_f1 =  f1_score(np.array(flatten_preds)[idx], np.array(flatten_gold)[idx], average='macro')\n",
    "    return micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=37)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in (range(epochs)):\n",
    "        print(\"Training Epoch {}\".format(epoch))\n",
    "        training_loss = train_one_epoch(model, trainDataLoader, optimizer, loss_function)\n",
    "        print(\"Training Loss: {}\".format(training_loss))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        train_preds = []\n",
    "        for batch in trainDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            train_preds.extend(predictions)\n",
    "        train_preds = np.array(train_preds, dtype=object)\n",
    "\n",
    "        train_micro_f1, train_macro_f1 = get_scores(train_preds, trainY)\n",
    "\n",
    "        val_preds = []\n",
    "        for batch in testDataLoader:\n",
    "            X, y, seq_lens, mask = batch\n",
    "            predictions = model.predict(X, mask)\n",
    "            val_preds.extend(predictions)\n",
    "        val_preds = np.array(val_preds, dtype=object)\n",
    "\n",
    "        val_micro_f1, val_macro_f1 = get_scores(val_preds, testY)\n",
    "\n",
    "        print(\"Training Micro F1: {}\".format(train_micro_f1))\n",
    "        print(\"Training Macro F1: {}\".format(train_macro_f1))\n",
    "        print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "        print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "\n",
    "        train_f1 = (train_micro_f1 + train_macro_f1) / 2\n",
    "        val_f1 = (val_micro_f1 + val_macro_f1) / 2\n",
    "\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            print(\"New Best Model at Epoch {}\".format(epoch))\n",
    "            print(\"Validation Micro F1: {}\".format(val_micro_f1))\n",
    "            print(\"Validation Macro F1: {}\".format(val_macro_f1))\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model_2.pt')\n",
    "        \n",
    "        if epoch>=best_epoch + 3:\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    return model, train_f1s, val_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = BiLSTMCRF(weights_matrix, 256, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:55<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 310.7379181780134\n",
      "Training Micro F1: 0.6343544163161654\n",
      "Training Macro F1: 0.41264621055031414\n",
      "Validation Micro F1: 0.6427086664534698\n",
      "Validation Macro F1: 0.4123955811075538\n",
      "New Best Model at Epoch 0\n",
      "Validation Micro F1: 0.6427086664534698\n",
      "Validation Macro F1: 0.4123955811075538\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:59<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 192.33611167907713\n",
      "Training Micro F1: 0.7005098801682406\n",
      "Training Macro F1: 0.5253626873677553\n",
      "Validation Micro F1: 0.6968340262232171\n",
      "Validation Macro F1: 0.5095334206126828\n",
      "New Best Model at Epoch 1\n",
      "Validation Micro F1: 0.6968340262232171\n",
      "Validation Macro F1: 0.5095334206126828\n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:59<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 150.2544627925328\n",
      "Training Micro F1: 0.7349714308388223\n",
      "Training Macro F1: 0.583660107278476\n",
      "Validation Micro F1: 0.7217780620402943\n",
      "Validation Macro F1: 0.5604412352900827\n",
      "New Best Model at Epoch 2\n",
      "Validation Micro F1: 0.7217780620402943\n",
      "Validation Macro F1: 0.5604412352900827\n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:00<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 126.44831134251186\n",
      "Training Micro F1: 0.757489484961511\n",
      "Training Macro F1: 0.6251558955522054\n",
      "Validation Micro F1: 0.7350095938599296\n",
      "Validation Macro F1: 0.5903636483428863\n",
      "New Best Model at Epoch 3\n",
      "Validation Micro F1: 0.7350095938599296\n",
      "Validation Macro F1: 0.5903636483428863\n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:00<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 111.4937015914917\n",
      "Training Micro F1: 0.7710796762161733\n",
      "Training Macro F1: 0.6562136489989597\n",
      "Validation Micro F1: 0.743404221298369\n",
      "Validation Macro F1: 0.6167652409869026\n",
      "New Best Model at Epoch 4\n",
      "Validation Micro F1: 0.743404221298369\n",
      "Validation Macro F1: 0.6167652409869026\n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:00<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 101.05636154174805\n",
      "Training Micro F1: 0.7829041345924926\n",
      "Training Macro F1: 0.6798896351740622\n",
      "Validation Micro F1: 0.7505996162456027\n",
      "Validation Macro F1: 0.6312226612308607\n",
      "New Best Model at Epoch 5\n",
      "Validation Micro F1: 0.7505996162456027\n",
      "Validation Macro F1: 0.6312226612308607\n",
      "Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:00<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 93.42610597882953\n",
      "Training Micro F1: 0.7941334021109436\n",
      "Training Macro F1: 0.7005520224246234\n",
      "Validation Micro F1: 0.7575151902782219\n",
      "Validation Macro F1: 0.6450174571156831\n",
      "New Best Model at Epoch 6\n",
      "Validation Micro F1: 0.7575151902782219\n",
      "Validation Macro F1: 0.6450174571156831\n",
      "Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:00<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 87.33163429805211\n",
      "Training Micro F1: 0.8050650742004603\n",
      "Training Macro F1: 0.7154195168475109\n",
      "Validation Micro F1: 0.7634713783178766\n",
      "Validation Macro F1: 0.6571003001607479\n",
      "New Best Model at Epoch 7\n",
      "Validation Micro F1: 0.7634713783178766\n",
      "Validation Macro F1: 0.6571003001607479\n",
      "Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:00<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 81.32386232921056\n",
      "Training Micro F1: 0.8176335211491151\n",
      "Training Macro F1: 0.7357932569995984\n",
      "Validation Micro F1: 0.7704669011832427\n",
      "Validation Macro F1: 0.6775158340258987\n",
      "New Best Model at Epoch 8\n",
      "Validation Micro F1: 0.7704669011832427\n",
      "Validation Macro F1: 0.6775158340258987\n",
      "Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:01<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 75.95323977606637\n",
      "Training Micro F1: 0.8325430521387192\n",
      "Training Macro F1: 0.7551776059283645\n",
      "Validation Micro F1: 0.7814998401023345\n",
      "Validation Macro F1: 0.687472727517887\n",
      "New Best Model at Epoch 9\n",
      "Validation Micro F1: 0.7814998401023345\n",
      "Validation Macro F1: 0.687472727517887\n",
      "Training Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:01<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 70.33803054264614\n",
      "Training Micro F1: 0.84638123958416\n",
      "Training Macro F1: 0.7778536965900456\n",
      "Validation Micro F1: 0.78621682123441\n",
      "Validation Macro F1: 0.6969765661699727\n",
      "New Best Model at Epoch 10\n",
      "Validation Micro F1: 0.78621682123441\n",
      "Validation Macro F1: 0.6969765661699727\n",
      "Training Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:01<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 64.55386378424508\n",
      "Training Micro F1: 0.8605071026109039\n",
      "Training Macro F1: 0.7963374399159747\n",
      "Validation Micro F1: 0.7931323952670291\n",
      "Validation Macro F1: 0.7086683563517763\n",
      "New Best Model at Epoch 11\n",
      "Validation Micro F1: 0.7931323952670291\n",
      "Validation Macro F1: 0.7086683563517763\n",
      "Training Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:01<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 58.90062255314418\n",
      "Training Micro F1: 0.871537973176732\n",
      "Training Macro F1: 0.8128664547524647\n",
      "Validation Micro F1: 0.7980492484809721\n",
      "Validation Macro F1: 0.7151359034869739\n",
      "New Best Model at Epoch 12\n",
      "Validation Micro F1: 0.7980492484809721\n",
      "Validation Macro F1: 0.7151359034869739\n",
      "Training Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 53.09438088825771\n",
      "Training Micro F1: 0.8830350765812237\n",
      "Training Macro F1: 0.8249241144045971\n",
      "Validation Micro F1: 0.8042452830188679\n",
      "Validation Macro F1: 0.7190095642139552\n",
      "New Best Model at Epoch 13\n",
      "Validation Micro F1: 0.8042452830188679\n",
      "Validation Macro F1: 0.7190095642139552\n",
      "Training Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:01<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 48.024720241001674\n",
      "Training Micro F1: 0.8972204586937544\n",
      "Training Macro F1: 0.8416888678180965\n",
      "Validation Micro F1: 0.8137591941157659\n",
      "Validation Macro F1: 0.7368723634871233\n",
      "New Best Model at Epoch 14\n",
      "Validation Micro F1: 0.8137591941157659\n",
      "Validation Macro F1: 0.7368723634871233\n",
      "Training Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 43.42842693192618\n",
      "Training Micro F1: 0.903122768034283\n",
      "Training Macro F1: 0.859017804798423\n",
      "Validation Micro F1: 0.8098017268947874\n",
      "Validation Macro F1: 0.735441646929043\n",
      "Training Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 39.19116806983948\n",
      "Training Micro F1: 0.9132509324656773\n",
      "Training Macro F1: 0.8674082758056034\n",
      "Validation Micro F1: 0.8171570195075151\n",
      "Validation Macro F1: 0.7398619472000773\n",
      "New Best Model at Epoch 16\n",
      "Validation Micro F1: 0.8171570195075151\n",
      "Validation Macro F1: 0.7398619472000773\n",
      "Training Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 35.48506118910653\n",
      "Training Micro F1: 0.918240615824141\n",
      "Training Macro F1: 0.8668569802369814\n",
      "Validation Micro F1: 0.8167173009274065\n",
      "Validation Macro F1: 0.7387254505979396\n",
      "Training Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 32.59913829667228\n",
      "Training Micro F1: 0.9274759939687325\n",
      "Training Macro F1: 0.8870442683726275\n",
      "Validation Micro F1: 0.8239126958746402\n",
      "Validation Macro F1: 0.7498467862112992\n",
      "New Best Model at Epoch 18\n",
      "Validation Micro F1: 0.8239126958746402\n",
      "Validation Macro F1: 0.7498467862112992\n",
      "Training Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 29.95822246823992\n",
      "Training Micro F1: 0.9363443377509721\n",
      "Training Macro F1: 0.8994534274553224\n",
      "Validation Micro F1: 0.8295091141669332\n",
      "Validation Macro F1: 0.7587550011212714\n",
      "New Best Model at Epoch 19\n",
      "Validation Micro F1: 0.8295091141669332\n",
      "Validation Macro F1: 0.7587550011212714\n",
      "Training Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 27.782852725982664\n",
      "Training Micro F1: 0.9366816125704309\n",
      "Training Macro F1: 0.8981314717286574\n",
      "Validation Micro F1: 0.8282299328429805\n",
      "Validation Macro F1: 0.7578757410396254\n",
      "Training Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 25.98751249585833\n",
      "Training Micro F1: 0.9412645821760178\n",
      "Training Macro F1: 0.9050213978622158\n",
      "Validation Micro F1: 0.8289494723377039\n",
      "Validation Macro F1: 0.7550934412221413\n",
      "Training Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:02<00:00, 11.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 24.385292972837174\n",
      "Training Micro F1: 0.9483076739941274\n",
      "Training Macro F1: 0.9154328775122739\n",
      "Validation Micro F1: 0.8318676047329709\n",
      "Validation Macro F1: 0.7553466651805627\n"
     ]
    }
   ],
   "source": [
    "ner, train_f1s, val_f1s = train_model(ner, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model\n",
    "ner.load_state_dict(torch.load('best_model_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ner, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMCRF(\n",
       "  (embedding): Embedding(7404, 50)\n",
       "  (lstm): LSTM(50, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=38, bias=True)\n",
       "  (crf): CRF(num_tags=38)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation predictions using valDataloader\n",
    "val_preds = []\n",
    "for batch in valDataLoader:\n",
    "    X, y, seq_lens, mask = batch\n",
    "    predictions = ner.predict(X, mask)\n",
    "    val_preds.extend(predictions)\n",
    "val_preds = np.array(val_preds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_val_preds = []\n",
    "flatten_valY = []\n",
    "for i in range(len(val_preds)):\n",
    "    for j in range(len(val_preds[i])):\n",
    "        flatten_val_preds.append(val_preds[i][j])\n",
    "        flatten_valY.append(valY[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6, 14, 15, 2, 3, 0, 0, 22, 0, 0, 26, 0]),\n",
       "       list([6, 2, 0, 0, 0, 6, 14, 15, 0, 16, 17, 2, 0, 0, 4, 20, 0, 16, 17, 17, 2, 0, 0, 0]),\n",
       "       list([6, 21, 2, 3, 3, 0, 2, 3, 0]), ...,\n",
       "       list([6, 0, 22, 0, 11, 12, 0, 6, 2, 0, 0, 22, 0]),\n",
       "       list([6, 4, 0, 1, 0, 6, 0, 22, 0]),\n",
       "       list([6, 0, 18, 19, 0, 11, 12, 0, 0, 0, 4, 6, 0, 6, 0, 2, 0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "with open('val_preds.txt', 'w') as f:\n",
    "    for i in range(len(val_preds)):\n",
    "        for j in range(len(val_preds[i])):\n",
    "            f.write(labels_inv[val_preds[i][j]] + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION Report\n",
      "                 precision    recall  f1_score  true_entities  pred_entities\n",
      "Reagent           0.786298  0.784423  0.785359         5033.0         5021.0\n",
      "Action            0.808380  0.795055  0.801662         3640.0         3580.0\n",
      "Modifier          0.463139  0.512513  0.486576         1998.0         2211.0\n",
      "Location          0.688480  0.640020  0.663366         1989.0         1849.0\n",
      "Amount            0.918056  0.769499  0.837239         1718.0         1440.0\n",
      "Time              0.885714  0.874431  0.880037         1099.0         1085.0\n",
      "Device            0.574341  0.530454  0.551526          903.0          834.0\n",
      "Method            0.463080  0.486696  0.474595          902.0          948.0\n",
      "Concentration     0.746121  0.747175  0.746648          708.0          709.0\n",
      "Temperature       0.865819  0.914925  0.889695          670.0          708.0\n",
      "Measure-Type      0.461977  0.565116  0.508368          430.0          526.0\n",
      "Generic-Measure   0.478992  0.190000  0.272076          300.0          119.0\n",
      "Speed             0.886525  0.836120  0.860585          299.0          282.0\n",
      "Numerical         0.380597  0.588462  0.462236          260.0          402.0\n",
      "Size              0.413953  0.402715  0.408257          221.0          215.0\n",
      "Seal              0.726496  0.674603  0.699588          126.0          117.0\n",
      "Mention           0.733333  0.511628  0.602740           86.0           60.0\n",
      "pH                0.983333  0.907692  0.944000           65.0           60.0\n",
      "micro_avg         0.717148  0.707292  0.712186        20447.0        20166.0\n",
      "macro_avg         0.681369  0.651752  0.659697        20447.0        20166.0\n",
      "weighted_avg      0.724133  0.707292  0.713456        20447.0        20166.0\n",
      "\n",
      "F scores\n",
      "Micro-F1 = 0.7121857533302144\n",
      "Macro-F1 = 0.6596973886946134\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py Data/dev.txt val_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = get_data(test_data)\n",
    "testData = []\n",
    "for i in range(len(testX)):\n",
    "    testData.append((testX[i], testY[i]))\n",
    "testData = np.array(testData, dtype=object)\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation predictions using valDataloader\n",
    "test_preds = []\n",
    "for batch in testDataLoader:\n",
    "    X, y, seq_lens, mask = batch\n",
    "    predictions = ner.predict(X, mask)\n",
    "    test_preds.extend(predictions)\n",
    "test_preds = np.array(test_preds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions to file\n",
    "with open('new_test_preds.txt', 'w') as f:\n",
    "    for i in range(len(test_preds)):\n",
    "        for j in range(len(test_preds[i])):\n",
    "            f.write(labels_inv[test_preds[i][j]] + '\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION Report\n",
      "                 precision    recall  f1_score  true_entities  pred_entities\n",
      "Reagent           0.854554  0.825050  0.839543         6459.0         6236.0\n",
      "Action            0.859139  0.827670  0.843111         4532.0         4366.0\n",
      "Modifier          0.553834  0.583835  0.568439         2326.0         2452.0\n",
      "Amount            0.930435  0.838635  0.882153         2169.0         1955.0\n",
      "Location          0.760563  0.756000  0.758275         2000.0         1988.0\n",
      "Time              0.940207  0.909145  0.924415         1695.0         1639.0\n",
      "Method            0.577120  0.521218  0.547746         1084.0          979.0\n",
      "Temperature       0.922027  0.917556  0.919786         1031.0         1026.0\n",
      "Concentration     0.829317  0.836032  0.832661          988.0          996.0\n",
      "Device            0.750000  0.665541  0.705251          888.0          788.0\n",
      "Measure-Type      0.555035  0.635389  0.592500          373.0          427.0\n",
      "Speed             0.892966  0.895706  0.894334          326.0          327.0\n",
      "Generic-Measure   0.483221  0.240803  0.321429          299.0          149.0\n",
      "Numerical         0.416327  0.682274  0.517110          299.0          490.0\n",
      "Size              0.576355  0.485477  0.527027          241.0          203.0\n",
      "pH                0.882353  0.789474  0.833333          133.0          119.0\n",
      "Seal              0.765957  0.631579  0.692308          114.0           94.0\n",
      "Mention           0.682927  0.474576  0.560000           59.0           41.0\n",
      "micro_avg         0.797734  0.774105  0.785742        25016.0        24275.0\n",
      "macro_avg         0.735130  0.695331  0.708857        25016.0        24275.0\n",
      "weighted_avg      0.801360  0.774105  0.786164        25016.0        24275.0\n",
      "\n",
      "F scores\n",
      "Micro-F1 = 0.7857418189933254\n",
      "Macro-F1 = 0.7088566958271527\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py Data/test.txt new_test_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
