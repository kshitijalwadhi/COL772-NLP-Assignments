{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kshitijalwadhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kshitijalwadhi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kshitijalwadhi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kshitijalwadhi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_deps():\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "    nltk.download(\"punkt\")\n",
    "    return True\n",
    "check_deps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"Data/train.csv\"\n",
    "NUM_FOLDS = 5\n",
    "FRACTION_DATA = 1\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df = df.sample(frac=FRACTION_DATA, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "professions = df[\"profession\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df, profession, target_num):\n",
    "    profiles = df[df[\"profession\"]==profession][\"profile\"].values\n",
    "    if len(profiles) > target_num:\n",
    "        return df\n",
    "    all_sentences = []\n",
    "    for profile in profiles:\n",
    "        sentences = tokenize.sent_tokenize(profile)\n",
    "        sentences = [sentence for sentence in sentences if len(sentence)>10]\n",
    "        all_sentences.extend(sentences)\n",
    "    new_profiles = []\n",
    "    np.random.shuffle(all_sentences)\n",
    "    NUM_SENTENCES = 4\n",
    "    for i in range(target_num - len(profiles)):\n",
    "        new_profile = \".\".join(np.random.choice(all_sentences, NUM_SENTENCES))\n",
    "        new_profiles.append(new_profile)\n",
    "    new_df = pd.DataFrame({\"profile\": new_profiles, \"profession\": profession})\n",
    "    combined_df = pd.concat([df, new_df])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    profiles = df[\"profile\"]\n",
    "    cleaned_profiles = []\n",
    "    for profile in tqdm(profiles):\n",
    "        profile = re.sub(r'[^a-zA-Z .]', ' ', profile)\n",
    "        profile = profile.lower()\n",
    "        profile = profile.split()\n",
    "        profile = [stemmer.stem(word) for word in profile if not word in set(stopwords)]\n",
    "        profile = [lemmatizer.lemmatize(word) for word in profile if not word in set(stopwords)]\n",
    "        profile = ' '.join(profile)\n",
    "        cleaned_profiles.append(profile)\n",
    "    df[\"profile\"] = cleaned_profiles\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(df, column):\n",
    "    df[column] = df[column].str.replace('[^\\w\\s]','')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df):\n",
    "    X, y = df.drop(\"profession\", axis=1), df[\"profession\"]\n",
    "    over = RandomOverSampler()\n",
    "    X_sampled, y_sampled = over.fit_resample(X, y)\n",
    "    X_sampled[\"profession\"] = y_sampled\n",
    "    return X_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127855/127855 [01:21<00:00, 1565.66it/s]\n"
     ]
    }
   ],
   "source": [
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=False)\n",
    "train_index, val_index = next(kf.split(df[\"profile\"], df[\"profession\"]))\n",
    "train_data = df.iloc[train_index]\n",
    "val_data = df.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:24<00:00,  1.13it/s]\n",
      "/var/folders/93/9kgm8vks4wz6zr8rg0d_pb7w0000gn/T/ipykernel_19849/3177127164.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column] = df[column].str.replace('[^\\w\\s]','')\n",
      "/var/folders/93/9kgm8vks4wz6zr8rg0d_pb7w0000gn/T/ipykernel_19849/3177127164.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "# Augment the data to increase count\n",
    "AUGMENT_DATA = True\n",
    "if AUGMENT_DATA:\n",
    "    max_num = int(max(train_data[\"profession\"].value_counts())/10)\n",
    "    for profession in tqdm(professions):\n",
    "        train_data = augment_data(train_data, profession, max_num)\n",
    "\n",
    "train_data = remove_punctuation(train_data, \"profile\")\n",
    "val_data = remove_punctuation(val_data, \"profile\")\n",
    "\n",
    "# Upsample data\n",
    "# train_data = sample_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<137223x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6070003 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FEATURES = 50000\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,2), max_features=MAX_FEATURES, sublinear_tf=True)\n",
    "vectorizer.fit_transform(train_data[\"profile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_and_y(data_df):\n",
    "    y = data_df[\"profession\"]\n",
    "    X_vec = vectorizer.transform(data_df[\"profile\"])\n",
    "    return X_vec, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "train_data = train_data.sample(frac=1, random_state = 42).reset_index(drop=True)\n",
    "val_data = val_data.sample(frac=1, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec, y_train = get_X_and_y(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNB = MultinomialNB(alpha=0.7).fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1:  0.7178053263462517\n",
      "Macro F1:  0.6442509562131045\n",
      "Average F1:  0.6810281412796781\n"
     ]
    }
   ],
   "source": [
    "X_val_vec, y_val = get_X_and_y(val_data)\n",
    "y_pred = modelNB.predict(X_val_vec)\n",
    "\n",
    "print(\"Micro F1: \", metrics.f1_score(y_val, y_pred, average='micro'))\n",
    "print(\"Macro F1: \", metrics.f1_score(y_val, y_pred, average='macro'))\n",
    "print(\"Average F1: \", (metrics.f1_score(y_val, y_pred, average='micro') + metrics.f1_score(y_val, y_pred, average='macro'))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "modelSVC = LinearSVC(C=0.1, penalty=\"l2\", dual=False, max_iter=10000).fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1:  0.8153767940244809\n",
      "Macro F1:  0.7511970234227462\n",
      "Average F1:  0.7832869087236136\n"
     ]
    }
   ],
   "source": [
    "X_val_vec, y_val = get_X_and_y(val_data)\n",
    "y_pred = modelSVC.predict(X_val_vec)\n",
    "\n",
    "print(\"Micro F1: \", metrics.f1_score(y_val, y_pred, average='micro'))\n",
    "print(\"Macro F1: \", metrics.f1_score(y_val, y_pred, average='macro'))\n",
    "print(\"Average F1: \", (metrics.f1_score(y_val, y_pred, average='micro') + metrics.f1_score(y_val, y_pred, average='macro'))/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
